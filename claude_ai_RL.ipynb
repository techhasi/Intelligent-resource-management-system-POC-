{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, BatchNormalization, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "import shutil\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = 'merged_cloud_metrics.csv'\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "# Define warmup steps (initial steps to skip)\n",
    "warmup_steps = 24\n",
    "current_step = warmup_steps\n",
    "\n",
    "# Define action and observation spaces\n",
    "n_actions_per_resource = 3\n",
    "n_resources = 3\n",
    "action_space_size = n_actions_per_resource ** n_resources\n",
    "observation_space_shape = (6,)  # 6 metrics: EC2_CPU, EC2_Memory, RDS_CPU, RDS_Memory, ECS_CPU, ECS_Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get the current state\n",
    "def get_state(data, step):\n",
    "    current_data = data.iloc[step]\n",
    "    return np.array([\n",
    "        current_data['EC2_CPUUtilization'],\n",
    "        current_data['EC2_MemoryUtilization'],\n",
    "        current_data['RDS_CPUUtilization'],\n",
    "        current_data['RDS_FreeableMemory'],\n",
    "        current_data['ECS_CPUUtilization'],\n",
    "        current_data['ECS_MemoryUtilization']\n",
    "    ], dtype=np.float32)\n",
    "\n",
    "# Function to decode action index into scaling actions\n",
    "def decode_action(action_idx):\n",
    "    actions = []\n",
    "    temp_idx = action_idx\n",
    "    for _ in range(n_resources):\n",
    "        resource_action = temp_idx % n_actions_per_resource\n",
    "        temp_idx //= n_actions_per_resource\n",
    "        actions.append(resource_action - 1)  # Convert to [-1, 0, 1]\n",
    "    return np.array(actions, dtype=np.float32)\n",
    "\n",
    "# Function to calculate reward\n",
    "def calculate_reward(data, step, action):\n",
    "    current_data = data.iloc[step]\n",
    "    cpu_penalty = -abs(current_data['EC2_CPUUtilization'] - 70)\n",
    "    memory_penalty = -abs(current_data['EC2_MemoryUtilization'] - 70)\n",
    "    scaling_penalty = -np.sum(np.abs(action)) * 10\n",
    "    performance_reward = 100 if (30 <= current_data['EC2_CPUUtilization'] <= 80) else -50\n",
    "    return cpu_penalty + memory_penalty + scaling_penalty + performance_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. DQN Agent Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "state_size = 6\n",
    "action_size = 27  # 3 actions per resource, 3 resources\n",
    "memory = deque(maxlen=2000)\n",
    "gamma = 0.95  # Discount rate\n",
    "epsilon = 1.0  # Exploration rate\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995\n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "target_update_frequency = 10  # Update target model every 10 episodes\n",
    "\n",
    "# Build the DQN model\n",
    "model = Sequential([\n",
    "    Dense(64, input_dim=state_size, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(action_size, activation='linear')\n",
    "])\n",
    "model.compile(loss='mse', optimizer=Adam(learning_rate=learning_rate))\n",
    "\n",
    "# Target model (for stability)\n",
    "target_model = Sequential([\n",
    "    Dense(64, input_dim=state_size, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(64, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(action_size, activation='linear')\n",
    "])\n",
    "target_model.set_weights(model.get_weights())\n",
    "\n",
    "# Learning rate scheduler\n",
    "reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=5, min_lr=0.0001)\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "# Function to update the target model\n",
    "def update_target_model():\n",
    "    target_model.set_weights(model.get_weights())\n",
    "\n",
    "# Function to choose an action\n",
    "def choose_action(state):\n",
    "    if np.random.rand() <= epsilon:\n",
    "        return np.random.randint(action_size)\n",
    "    act_values = model.predict(state, verbose=0)\n",
    "    return int(np.argmax(act_values[0]))\n",
    "\n",
    "# Function to replay experiences with prioritized experience replay\n",
    "def replay_experiences():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    minibatch = random.sample(memory, batch_size)\n",
    "    states = np.zeros((batch_size, state_size))\n",
    "    targets = np.zeros((batch_size, action_size))\n",
    "\n",
    "    for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "        target = reward if done else reward + gamma * np.amax(target_model.predict(next_state, verbose=0)[0])\n",
    "        target_f = model.predict(state, verbose=0)[0]\n",
    "        target_f[action] = target\n",
    "        states[i] = state[0]\n",
    "        targets[i] = target_f\n",
    "\n",
    "    model.fit(states, targets, epochs=1, verbose=0, batch_size=batch_size, callbacks=[reduce_lr, early_stopping])\n",
    "\n",
    "    # Decay epsilon\n",
    "    global epsilon\n",
    "    if epsilon > epsilon_min:\n",
    "        epsilon *= epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     22\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mreplay_experiences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m episode \u001b[38;5;241m%\u001b[39m target_update_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     27\u001b[0m     update_target_model()\n",
      "Cell \u001b[0;32mIn[13], line 63\u001b[0m, in \u001b[0;36mreplay_experiences\u001b[0;34m()\u001b[0m\n\u001b[1;32m     60\u001b[0m targets \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros((batch_size, action_size))\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (state, action, reward, next_state, done) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(minibatch):\n\u001b[0;32m---> 63\u001b[0m     target \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m reward \u001b[38;5;241m+\u001b[39m gamma \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mamax(\u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     64\u001b[0m     target_f \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(state, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     65\u001b[0m     target_f[action] \u001b[38;5;241m=\u001b[39m target\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:559\u001b[0m, in \u001b[0;36mTensorFlowTrainer.predict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    557\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[0;32m--> 559\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    560\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_predict_batch_begin(step)\n\u001b[1;32m    561\u001b[0m         data \u001b[38;5;241m=\u001b[39m get_data(iterator)\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:736\u001b[0m, in \u001b[0;36mTFEpochIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_epoch_iterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/keras/src/trainers/epoch_iterator.py:112\u001b[0m, in \u001b[0;36mEpochIterator._enumerate_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m step, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_batches:\n\u001b[0;32m--> 112\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_current_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_steps_seen \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/dataset_ops.py:501\u001b[0m, in \u001b[0;36mDatasetV2.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mor\u001b[39;00m ops\u001b[38;5;241m.\u001b[39minside_function():\n\u001b[1;32m    500\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[0;32m--> 501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43miterator_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mOwnedIterator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    503\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`tf.data.Dataset` only supports Python-style \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    504\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miteration in eager mode or within tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:705\u001b[0m, in \u001b[0;36mOwnedIterator.__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    701\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (components \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m element_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    702\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhen `dataset` is provided, `element_spec` and `components` must \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    704\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot be specified.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 705\u001b[0m   \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next_call_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/tensorflow/python/data/ops/iterator_ops.py:744\u001b[0m, in \u001b[0;36mOwnedIterator._create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    741\u001b[0m   \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fulltype\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39margs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(\n\u001b[1;32m    742\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_output_types)\n\u001b[1;32m    743\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator_resource\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mexperimental_set_type(fulltype)\n\u001b[0;32m--> 744\u001b[0m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_variant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator_resource\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/tensorflow/python/ops/gen_dataset_ops.py:3478\u001b[0m, in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   3476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   3477\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3478\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3479\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMakeIterator\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   3481\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "episodes = 1000\n",
    "for episode in range(episodes):\n",
    "    state = get_state(df, current_step)\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    total_reward = 0\n",
    "\n",
    "    for time in range(len(df) - warmup_steps - 1):\n",
    "        action_idx = choose_action(state)\n",
    "        action = decode_action(action_idx)\n",
    "        next_state = get_state(df, current_step + 1)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        reward = calculate_reward(df, current_step, action)\n",
    "        done = current_step >= len(df) - 2\n",
    "\n",
    "        memory.append((state, action_idx, reward, next_state, done))\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        current_step += 1\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "        replay_experiences()\n",
    "\n",
    "    if episode % target_update_frequency == 0:\n",
    "        update_target_model()\n",
    "\n",
    "    print(f\"Episode: {episode + 1}/{episodes}, Total Reward: {total_reward}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model.save('cloud_rl_model.h5')\n",
    "\n",
    "# Move the model to the desired directory\n",
    "# shutil.move('cloud_rl_model.h5', '/content/drive/MyDrive/FYPDataset/')\n",
    "# print(\"Model saved and moved to the desired directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the trained model\n",
    "model = tf.keras.models.load_model('cloud_rl_model.h5')\n",
    "\n",
    "# Reset environment\n",
    "current_step = warmup_steps\n",
    "state = get_state(df, current_step)\n",
    "state = np.reshape(state, [1, state_size])\n",
    "\n",
    "# Lists to store results\n",
    "actual_actions = []\n",
    "predicted_actions = []\n",
    "rewards = []\n",
    "cpu_utilization = []\n",
    "memory_utilization = []\n",
    "\n",
    "# Simulate the environment using the trained model\n",
    "for time in range(len(df) - warmup_steps - 1):\n",
    "    # Get the actual action (ground truth, if available)\n",
    "    # For demonstration, assume the optimal action is to do nothing (action index = 1)\n",
    "    actual_action = 1  # Replace with actual ground truth if available\n",
    "    actual_actions.append(actual_action)\n",
    "\n",
    "    # Predict action using the trained model\n",
    "    action_idx = np.argmax(model.predict(state, verbose=0)[0])\n",
    "    predicted_actions.append(action_idx)\n",
    "\n",
    "    # Decode the predicted action\n",
    "    action = decode_action(action_idx)\n",
    "\n",
    "    # Take the action and get the next state\n",
    "    next_state = get_state(df, current_step + 1)\n",
    "    next_state = np.reshape(next_state, [1, state_size])\n",
    "    reward = calculate_reward(df, current_step, action)\n",
    "\n",
    "    # Store results\n",
    "    rewards.append(reward)\n",
    "    cpu_utilization.append(df.iloc[current_step]['EC2_CPUUtilization'])\n",
    "    memory_utilization.append(df.iloc[current_step]['EC2_MemoryUtilization'])\n",
    "\n",
    "    # Update state and step\n",
    "    state = next_state\n",
    "    current_step += 1\n",
    "\n",
    "    # Stop if done\n",
    "    if current_step >= len(df) - 2:\n",
    "        break\n",
    "\n",
    "# Calculate accuracy (if ground truth is available)\n",
    "if len(actual_actions) > 0:\n",
    "    accuracy = np.mean(np.array(actual_actions) == np.array(predicted_actions))\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# Plot Actual vs. Predicted Actions\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(actual_actions, label='Actual Actions', marker='o')\n",
    "plt.plot(predicted_actions, label='Predicted Actions', marker='x')\n",
    "plt.title('Actual vs. Predicted Actions')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Action')\n",
    "plt.legend()\n",
    "\n",
    "# Plot CPU Utilization\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(cpu_utilization, label='CPU Utilization', color='orange')\n",
    "plt.title('CPU Utilization Over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('CPU Utilization (%)')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Memory Utilization\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(memory_utilization, label='Memory Utilization', color='green')\n",
    "plt.title('Memory Utilization Over Time')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Memory Utilization (%)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print average reward\n",
    "print(f\"Average Reward: {np.mean(rewards):.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
