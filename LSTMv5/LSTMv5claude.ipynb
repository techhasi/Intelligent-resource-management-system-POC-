{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "==================================================\n",
      "Processing EC2 service\n",
      "==================================================\n",
      "Loading data from ../dataset scripts/ec2_metrics.csv\n",
      "Shape after preprocessing: (43200, 4)\n",
      "Low utilization samples are underrepresented (8.6%). Augmenting data...\n",
      "Augmenting data with 17886 synthetic low-utilization samples...\n",
      "Data distribution after augmentation:\n",
      "  Low CPU (0-20%): 21600 samples (35.4%)\n",
      "  Medium CPU (20-70%): 36648 samples (60.0%)\n",
      "  High CPU (70-100%): 2838 samples (4.6%)\n",
      "Adding features for ec2...\n",
      "Dropped 3 rows containing NaN values\n",
      "Shape after feature engineering: (61083, 31)\n",
      "Added 27 new features\n",
      "\n",
      "--------------------------------------------------\n",
      "Processing EC2 data\n",
      "--------------------------------------------------\n",
      "Loading data from ../dataset scripts/ec2_metrics.csv\n",
      "Shape after preprocessing: (43200, 4)\n",
      "Adding features for ec2...\n",
      "Dropped 3 rows containing NaN values\n",
      "Shape after feature engineering: (43197, 31)\n",
      "Added 27 new features\n",
      "Data distribution:\n",
      "  Low CPU (0-20%): 3714 samples (8.6%)\n",
      "  Medium CPU (20-70%): 36645 samples (84.8%)\n",
      "  High CPU (70-100%): 2838 samples (6.6%)\n",
      "Preparing data with sequence length 12...\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import os\n",
    "import pickle\n",
    "import warnings\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "os.makedirs(\"plots\", exist_ok=True)\n",
    "\n",
    "# Function to load and preprocess data\n",
    "def load_and_preprocess(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loading data from {file_path}\")\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df = df.set_index('timestamp')\n",
    "    df = df.sort_index()\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "    \n",
    "    # Smooth CPU utilization with rolling window\n",
    "    for col in df.columns:\n",
    "        if 'CPUUtilization' in col:\n",
    "            df[col] = df[col].rolling(window=3, min_periods=1).mean()\n",
    "    \n",
    "    print(f\"Shape after preprocessing: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Feature engineering\n",
    "def add_features(df, service_type):\n",
    "    print(f\"Adding features for {service_type}...\")\n",
    "    \n",
    "    if service_type == 'ec2':\n",
    "        cpu_col = 'EC2_CPUUtilization'\n",
    "        memory_col = 'EC2_MemoryUtilization'\n",
    "        network_col = 'EC2_NetworkIn'\n",
    "        disk_col = 'EC2_DiskWriteOps'\n",
    "    elif service_type == 'rds':\n",
    "        cpu_col = 'RDS_CPUUtilization'\n",
    "        memory_col = 'RDS_FreeableMemory'\n",
    "        conn_col = 'RDS_DatabaseConnections'\n",
    "        io_col = 'RDS_WriteIOPS'\n",
    "    elif service_type == 'ecs':\n",
    "        cpu_col = 'ECS_CPUUtilization'\n",
    "        memory_col = 'ECS_MemoryUtilization'\n",
    "        task_col = 'ECS_RunningTaskCount'\n",
    "        network_col = 'ECS_NetworkIn'\n",
    "    \n",
    "    # Original features\n",
    "    original_columns = df.columns.tolist()\n",
    "    \n",
    "    # Add time-based features\n",
    "    # Ensure the index is a DatetimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "    \n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] < 17) & (df['is_weekend'] == 0)).astype(int)\n",
    "    \n",
    "    # Create utilization level categorical feature (will be one-hot encoded)\n",
    "    # This helps the model distinguish between low, medium, and high utilization patterns\n",
    "    def get_utilization_level(cpu):\n",
    "        if cpu <= 20:\n",
    "            return 0  # low\n",
    "        elif cpu <= 70:\n",
    "            return 1  # medium\n",
    "        else:\n",
    "            return 2  # high\n",
    "    \n",
    "    df['utilization_level'] = df[cpu_col].apply(get_utilization_level)\n",
    "    df['is_low_utilization'] = (df['utilization_level'] == 0).astype(int)\n",
    "    df['is_medium_utilization'] = (df['utilization_level'] == 1).astype(int)\n",
    "    df['is_high_utilization'] = (df['utilization_level'] == 2).astype(int)\n",
    "    \n",
    "    # Add lag features\n",
    "    for i in range(1, 4):\n",
    "        df[f'{cpu_col}_lag_{i}'] = df[cpu_col].shift(i)\n",
    "\n",
    "    # Add differencing features\n",
    "    df[f'{cpu_col}_diff_1'] = df[cpu_col].diff(1)\n",
    "    df[f'{cpu_col}_diff_3'] = df[cpu_col].diff(3)\n",
    "\n",
    "    # Add rolling statistics - more fine-grained windows\n",
    "    df[f'{cpu_col}_roll_mean_30m'] = df[cpu_col].rolling('30min', min_periods=1).mean()\n",
    "    df[f'{cpu_col}_roll_mean_1h'] = df[cpu_col].rolling('1H', min_periods=1).mean()\n",
    "    df[f'{cpu_col}_roll_std_1h'] = df[cpu_col].rolling('1H', min_periods=1).std().fillna(0)\n",
    "    \n",
    "    # Add rate of change features (acceleration/deceleration of CPU usage)\n",
    "    df[f'{cpu_col}_roc_5m'] = df[cpu_col].pct_change(periods=1)\n",
    "    df[f'{cpu_col}_roc_15m'] = df[cpu_col].pct_change(periods=3)\n",
    "    \n",
    "    # Service-specific features\n",
    "    if service_type == 'ec2':\n",
    "        df['EC2_CPU_Memory_Ratio'] = df[cpu_col] / (df[memory_col] + 1e-8)\n",
    "        df['EC2_CPU_Network_Ratio'] = df[cpu_col] / (df[network_col] + 1e-8)\n",
    "        df['EC2_CPU_Disk_Ratio'] = df[cpu_col] / (df[disk_col] + 1e-8)\n",
    "        \n",
    "        # New features for EC2\n",
    "        df['EC2_is_high_memory'] = (df[memory_col] > 70).astype(int)\n",
    "        df['EC2_memory_to_cpu_diff'] = df[memory_col] - df[cpu_col]\n",
    "        \n",
    "    elif service_type == 'rds':\n",
    "        df['RDS_Memory_Usage_Estimate'] = 100 - (df[memory_col] / df[memory_col].max() * 100)\n",
    "        df['RDS_CPU_Memory_Ratio'] = df[cpu_col] / (df['RDS_Memory_Usage_Estimate'] + 1e-8)\n",
    "        df['RDS_CPU_Conn_Ratio'] = df[cpu_col] / (df[conn_col] + 1e-8)\n",
    "        df['RDS_CPU_IO_Ratio'] = df[cpu_col] / (df[io_col] + 1e-8)\n",
    "        \n",
    "        # New features for RDS\n",
    "        df['RDS_conn_per_cpu'] = df[conn_col] / (df[cpu_col] + 1e-8)\n",
    "        df['RDS_io_intensive'] = (df[io_col] > df[io_col].quantile(0.75)).astype(int)\n",
    "        \n",
    "    elif service_type == 'ecs':\n",
    "        df['ECS_CPU_Memory_Ratio'] = df[cpu_col] / (df[memory_col] + 1e-8)\n",
    "        df['ECS_CPU_Task_Ratio'] = df[cpu_col] / (df[task_col] + 1e-8)\n",
    "        df['ECS_CPU_Network_Ratio'] = df[cpu_col] / (df[network_col] + 1e-8)\n",
    "        \n",
    "        # New features for ECS\n",
    "        df['ECS_per_task_cpu'] = df[cpu_col] / (df[task_col] + 1e-8)\n",
    "        df['ECS_is_memory_bound'] = (df[memory_col] > df[cpu_col]).astype(int)\n",
    "    \n",
    "    # Cyclic encoding of time features (better than raw values for time)\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Drop rows with NaN values (from lag features)\n",
    "    original_len = len(df)\n",
    "    df = df.dropna()\n",
    "    print(f\"Dropped {original_len - len(df)} rows containing NaN values\")\n",
    "    print(f\"Shape after feature engineering: {df.shape}\")\n",
    "    \n",
    "    # Save column names for later use\n",
    "    new_columns = [col for col in df.columns if col not in original_columns and col != cpu_col]\n",
    "    print(f\"Added {len(new_columns)} new features\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Custom weighted MSE loss\n",
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self, low_weight=3.0, med_weight=1.5, high_weight=1.0, threshold_low=20, threshold_high=70):\n",
    "        super(WeightedMSELoss, self).__init__()\n",
    "        self.low_weight = low_weight\n",
    "        self.med_weight = med_weight \n",
    "        self.high_weight = high_weight\n",
    "        self.threshold_low = threshold_low\n",
    "        self.threshold_high = threshold_high\n",
    "        \n",
    "    def forward(self, predictions, targets, cpu_values=None):\n",
    "        if cpu_values is None:\n",
    "            # If CPU values aren't provided, use the target values\n",
    "            cpu_values = targets\n",
    "            \n",
    "        # Calculate weights based on CPU values\n",
    "        weights = torch.ones_like(cpu_values)\n",
    "        weights = torch.where(cpu_values <= self.threshold_low, torch.tensor(self.low_weight).to(cpu_values.device), weights)\n",
    "        weights = torch.where((cpu_values > self.threshold_low) & (cpu_values <= self.threshold_high), \n",
    "                              torch.tensor(self.med_weight).to(cpu_values.device), weights)\n",
    "        \n",
    "        # Calculate MSE\n",
    "        squared_diff = (predictions - targets) ** 2\n",
    "        \n",
    "        # Apply weights\n",
    "        weighted_squared_diff = squared_diff * weights\n",
    "        \n",
    "        # Return mean\n",
    "        return weighted_squared_diff.mean()\n",
    "\n",
    "# Dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y, original_values=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.original_values = original_values  # For weighted loss\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.original_values is not None:\n",
    "            return self.X[idx], self.y[idx], self.original_values[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Improved LSTM Model with residual connections and layer normalization\n",
    "class EnhancedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(EnhancedLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Layer normalization (helps training)\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers with residual connection\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))  # lstm_out: batch_size, seq_len, hidden_size\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        lstm_out = self.layer_norm1(lstm_out)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Fully connected layers with residual connection\n",
    "        out = self.fc1(context)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        residual = out\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + residual  # Residual connection\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Data preparation with stratified sampling to ensure balanced utilization levels\n",
    "def prepare_data_for_lstm(df, target_col, sequence_length=12):\n",
    "    print(f\"Preparing data with sequence length {sequence_length}...\")\n",
    "    \n",
    "    # Extract target data\n",
    "    target_data = df[target_col].values.reshape(-1, 1)\n",
    "    \n",
    "    # Keep original CPU values for weighted loss\n",
    "    original_cpu_values = df[target_col].values\n",
    "    \n",
    "    # Identify utilization levels for stratified sampling\n",
    "    utilization_levels = df['utilization_level'].values\n",
    "    \n",
    "    # Scale data\n",
    "    feature_scaler = StandardScaler()\n",
    "    target_scaler = StandardScaler()\n",
    "    \n",
    "    # Scale target\n",
    "    scaled_target = target_scaler.fit_transform(target_data)\n",
    "    \n",
    "    # Scale features but keep target column unscaled for reference\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[target_col] = scaled_target.flatten()\n",
    "    feature_columns = [col for col in df.columns if col != target_col]\n",
    "    df_scaled[feature_columns] = feature_scaler.fit_transform(df[feature_columns])\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y, original_y, levels = [], [], [], []\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        X.append(df_scaled.values[i:i+sequence_length, :])\n",
    "        y.append(scaled_target[i+sequence_length])\n",
    "        original_y.append(original_cpu_values[i+sequence_length])\n",
    "        levels.append(utilization_levels[i+sequence_length])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    original_y = np.array(original_y)\n",
    "    levels = np.array(levels)\n",
    "    \n",
    "    # Stratified split to ensure balanced representation of utilization levels\n",
    "    X_train, X_val, y_train, y_val, original_y_train, original_y_val, levels_train, levels_val = train_test_split(\n",
    "        X, y, original_y, levels, test_size=0.2, stratify=levels, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    original_y_train = torch.tensor(original_y_train, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "    \n",
    "    X_val = torch.tensor(X_val, dtype=torch.float32).to(device)\n",
    "    y_val = torch.tensor(y_val, dtype=torch.float32).to(device)\n",
    "    original_y_val = torch.tensor(original_y_val, dtype=torch.float32).reshape(-1, 1).to(device)\n",
    "\n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Validation data shape: {X_val.shape}\")\n",
    "    \n",
    "    # Distribution of utilization levels\n",
    "    train_low = np.sum(levels_train == 0)\n",
    "    train_med = np.sum(levels_train == 1)\n",
    "    train_high = np.sum(levels_train == 2)\n",
    "    print(f\"Training data distribution - Low: {train_low} ({train_low/len(levels_train):.1%}), \" + \n",
    "          f\"Medium: {train_med} ({train_med/len(levels_train):.1%}), \" + \n",
    "          f\"High: {train_high} ({train_high/len(levels_train):.1%})\")\n",
    "    \n",
    "    val_low = np.sum(levels_val == 0)\n",
    "    val_med = np.sum(levels_val == 1)\n",
    "    val_high = np.sum(levels_val == 2)\n",
    "    print(f\"Validation data distribution - Low: {val_low} ({val_low/len(levels_val):.1%}), \" + \n",
    "          f\"Medium: {val_med} ({val_med/len(levels_val):.1%}), \" + \n",
    "          f\"High: {val_high} ({val_high/len(levels_val):.1%})\")\n",
    "    \n",
    "    all_columns = list(df_scaled.columns)\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, original_y_train, original_y_val, feature_scaler, target_scaler, all_columns\n",
    "\n",
    "# Training function with weighted loss\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=100, patience=10, scheduler=None, use_weighted_loss=True):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    counter = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for data in train_loader:\n",
    "            if use_weighted_loss and len(data) == 3:\n",
    "                inputs, targets, original_targets = data\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets, original_targets)\n",
    "            else:\n",
    "                inputs, targets = data[:2]  # Take first two elements\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for data in val_loader:\n",
    "                if use_weighted_loss and len(data) == 3:\n",
    "                    inputs, targets, original_targets = data\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets, original_targets)\n",
    "                else:\n",
    "                    inputs, targets = data[:2]\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                \n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        if scheduler:\n",
    "            scheduler.step(epoch_val_loss)\n",
    "        \n",
    "        # Early stopping\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "        \n",
    "        # Print progress\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_train_loss:.4f}, Val Loss: {epoch_val_loss:.4f}')\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, test_loader, criterion, target_scaler, use_weighted_loss=True):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    original_actuals = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            if use_weighted_loss and len(data) == 3:\n",
    "                inputs, targets, original_targets = data\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets, original_targets)\n",
    "                original_actuals.extend(original_targets.cpu().numpy())\n",
    "            else:\n",
    "                inputs, targets = data[:2]\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "            \n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            \n",
    "            # Store predictions and actuals\n",
    "            predictions.extend(outputs.cpu().numpy())\n",
    "            actuals.extend(targets.cpu().numpy())\n",
    "    \n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    # Unscale the data\n",
    "    predictions_unscaled = target_scaler.inverse_transform(predictions)\n",
    "    actuals_unscaled = target_scaler.inverse_transform(actuals)\n",
    "    \n",
    "    # Ensure predictions don't exceed 100% CPU utilization or go below 0%\n",
    "    predictions_unscaled = np.clip(predictions_unscaled, 0, 100)\n",
    "    \n",
    "    # Calculate metrics on unscaled data\n",
    "    all_mae = np.mean(np.abs(predictions_unscaled - actuals_unscaled))\n",
    "    all_rmse = np.sqrt(np.mean((predictions_unscaled - actuals_unscaled) ** 2))\n",
    "    \n",
    "    # Calculate metrics by utilization level\n",
    "    low_mask = actuals_unscaled <= 20\n",
    "    med_mask = (actuals_unscaled > 20) & (actuals_unscaled <= 70)\n",
    "    high_mask = actuals_unscaled > 70\n",
    "    \n",
    "    low_mae = np.mean(np.abs(predictions_unscaled[low_mask] - actuals_unscaled[low_mask])) if np.any(low_mask) else 0\n",
    "    med_mae = np.mean(np.abs(predictions_unscaled[med_mask] - actuals_unscaled[med_mask])) if np.any(med_mask) else 0\n",
    "    high_mae = np.mean(np.abs(predictions_unscaled[high_mask] - actuals_unscaled[high_mask])) if np.any(high_mask) else 0\n",
    "    \n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Overall MAE: {all_mae:.4f}, RMSE: {all_rmse:.4f}\")\n",
    "    print(f\"Low Utilization MAE: {low_mae:.4f} ({np.sum(low_mask)} samples)\")\n",
    "    print(f\"Medium Utilization MAE: {med_mae:.4f} ({np.sum(med_mask)} samples)\")\n",
    "    print(f\"High Utilization MAE: {high_mae:.4f} ({np.sum(high_mask)} samples)\")\n",
    "    \n",
    "    detailed_metrics = {\n",
    "        'mae': all_mae,\n",
    "        'rmse': all_rmse,\n",
    "        'low_mae': low_mae,\n",
    "        'med_mae': med_mae,\n",
    "        'high_mae': high_mae,\n",
    "        'low_count': np.sum(low_mask),\n",
    "        'med_count': np.sum(med_mask),\n",
    "        'high_count': np.sum(high_mask)\n",
    "    }\n",
    "    \n",
    "    return predictions_unscaled, actuals_unscaled, detailed_metrics\n",
    "\n",
    "# Function to plot training results and predictions\n",
    "def plot_results(train_losses, val_losses, predictions, actuals, metrics, service_name):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f'{service_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot predictions vs actuals\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Only plot a subset of points for clarity if there are many\n",
    "    max_points = 500\n",
    "    if len(predictions) > max_points:\n",
    "        step = len(predictions) // max_points\n",
    "        indices = range(0, len(predictions), step)\n",
    "        plot_predictions = predictions.flatten()[indices]\n",
    "        plot_actuals = actuals.flatten()[indices]\n",
    "    else:\n",
    "        plot_predictions = predictions.flatten()\n",
    "        plot_actuals = actuals.flatten()\n",
    "    \n",
    "    plt.plot(plot_actuals, label='Actual', alpha=0.7)\n",
    "    plt.plot(plot_predictions, label='Predicted', alpha=0.7)\n",
    "    plt.title(f'{service_name} - Predictions vs Actuals\\nMAE: {metrics[\"mae\"]:.4f}, RMSE: {metrics[\"rmse\"]:.4f}\\n' +\n",
    "              f'Low MAE: {metrics[\"low_mae\"]:.4f}, Med MAE: {metrics[\"med_mae\"]:.4f}, High MAE: {metrics[\"high_mae\"]:.4f}')\n",
    "    plt.xlabel('Sample')\n",
    "    plt.ylabel('CPU Utilization (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add a horizontal lines at 0% and 100% CPU\n",
    "    plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "    plt.axhline(y=100, color='r', linestyle='--', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/{service_name}_results.png')\n",
    "    \n",
    "    # Create a second plot for error analysis\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Calculate errors\n",
    "    errors = predictions.flatten() - actuals.flatten()\n",
    "    \n",
    "    # Plot error distribution\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(errors, bins=50, alpha=0.75)\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.title(f'{service_name} - Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot predictions vs actuals as scatter plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    \n",
    "    # Color points by utilization level\n",
    "    colors = np.zeros_like(actuals.flatten(), dtype=int)\n",
    "    colors[actuals.flatten() <= 20] = 0  # Low - blue\n",
    "    colors[(actuals.flatten() > 20) & (actuals.flatten() <= 70)] = 1  # Medium - green\n",
    "    colors[actuals.flatten() > 70] = 2  # High - red\n",
    "    \n",
    "    cmap = plt.cm.get_cmap('viridis', 3)\n",
    "    scatter = plt.scatter(actuals.flatten(), predictions.flatten(), c=colors, cmap=cmap, alpha=0.3)\n",
    "    \n",
    "    plt.plot([0, 100], [0, 100], 'r--')  # Perfect prediction line\n",
    "    plt.title(f'{service_name} - Predicted vs Actual')\n",
    "    plt.xlabel('Actual CPU Utilization (%)')\n",
    "    plt.ylabel('Predicted CPU Utilization (%)')\n",
    "    plt.xlim(0, 100)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add legend\n",
    "    from matplotlib.lines import Line2D\n",
    "    legend_elements = [\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(0), markersize=10, label='Low Utilization'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(1), markersize=10, label='Medium Utilization'),\n",
    "        Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(2), markersize=10, label='High Utilization')\n",
    "    ]\n",
    "    plt.legend(handles=legend_elements)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'plots/{service_name}_error_analysis.png')\n",
    "\n",
    "# Function to test predictions at different utilization levels\n",
    "def test_model_at_different_levels(model, feature_scaler, target_scaler, df, target_col, sequence_length):\n",
    "    # Prepare data\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    # Create copies of the dataset for each level\n",
    "    df_low = df.copy()\n",
    "    df_medium = df.copy()\n",
    "    df_high = df.copy()\n",
    "    \n",
    "    # Set target column to different utilization levels\n",
    "    df_low[target_col] = df_low[target_col].apply(lambda x: min(20, x))  # Set to low (0-20%)\n",
    "    df_medium[target_col] = df_medium[target_col].apply(lambda x: min(60, max(40, x)))  # Set to medium (40-60%)\n",
    "    df_high[target_col] = df_high[target_col].apply(lambda x: max(80, x))  # Set to high (80-100%)\n",
    "    \n",
    "    # Update feature engineering based on new target values\n",
    "    service_type = 'ec2' if 'EC2' in target_col else 'rds' if 'RDS' in target_col else 'ecs'\n",
    "    \n",
    "    df_low = add_features(df_low, service_type)\n",
    "    df_medium = add_features(df_medium, service_type)\n",
    "    df_high = add_features(df_high, service_type)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Test on each level\n",
    "    for level, df_test in [('low', df_low), ('medium', df_medium), ('high', df_high)]:\n",
    "        print(f\"\\nTesting on {level} utilization...\")\n",
    "        \n",
    "        # Scale features\n",
    "        feature_columns = list(df_test.columns)\n",
    "        if target_col in feature_columns:\n",
    "            feature_columns.remove(target_col)\n",
    "        \n",
    "        df_test_scaled = df_test.copy()\n",
    "        df_test_scaled[feature_columns] = feature_scaler.transform(df_test[feature_columns])\n",
    "        \n",
    "        # Prepare sequences\n",
    "        X_sequences = []\n",
    "        y_actual = []\n",
    "        \n",
    "        for i in range(min(100, len(df_test) - sequence_length)):  # Limit to 100 samples for testing\n",
    "            X_sequences.append(df_test_scaled.values[i:i+sequence_length, :])\n",
    "            y_actual.append(df_test[target_col].values[i+sequence_length])\n",
    "        \n",
    "        X_sequences = np.array(X_sequences)\n",
    "        X_sequences_tensor = torch.tensor(X_sequences, dtype=torch.float32).to(device)\n",
    "        \n",
    "        # Make predictions\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions_scaled = model(X_sequences_tensor).cpu().numpy()\n",
    "        \n",
    "        # Inverse transform predictions and clip to valid range\n",
    "        predictions = target_scaler.inverse_transform(predictions_scaled)\n",
    "        predictions = np.clip(predictions, 0, 100)  # Ensure predictions are between 0 and 100%\n",
    "        \n",
    "        # Calculate errors\n",
    "        y_actual = np.array(y_actual).reshape(-1, 1)\n",
    "        errors = np.abs(predictions - y_actual)\n",
    "        \n",
    "        avg_error = np.mean(errors)\n",
    "        max_error = np.max(errors)\n",
    "        \n",
    "        # Calculate percentage errors\n",
    "        percentage_errors = (errors / np.maximum(y_actual, 1e-8)) * 100\n",
    "        avg_percentage_error = np.mean(percentage_errors)\n",
    "        max_percentage_error = np.max(percentage_errors)\n",
    "        \n",
    "        results[level] = {\n",
    "            'avg_error': avg_error,\n",
    "            'max_error': max_error,\n",
    "            'avg_percentage_error': avg_percentage_error,\n",
    "            'max_percentage_error': max_percentage_error\n",
    "        }\n",
    "        \n",
    "        print(f\"Average error: {avg_error:.2f}%, Max error: {max_error:.2f}%\")\n",
    "        print(f\"Average percentage error: {avg_percentage_error:.2f}%, Max percentage error: {max_percentage_error:.2f}%\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Main pipeline function with enhanced features\n",
    "def run_lstm_pipeline(data_path, service_type, target_col):\n",
    "    print(f\"\\n{'-'*50}\")\n",
    "    print(f\"Processing {service_type.upper()} data\")\n",
    "    print(f\"{'-'*50}\")\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess data\n",
    "        df = load_and_preprocess(data_path)\n",
    "        \n",
    "        # Add features with enhanced engineering\n",
    "        df = add_features(df, service_type)\n",
    "        \n",
    "        # Print low, medium, high distribution\n",
    "        if 'CPUUtilization' in target_col:\n",
    "            low = len(df[df[target_col] <= 20])\n",
    "            medium = len(df[(df[target_col] > 20) & (df[target_col] <= 70)])\n",
    "            high = len(df[df[target_col] > 70])\n",
    "            total = len(df)\n",
    "            \n",
    "            print(f\"Data distribution:\")\n",
    "            print(f\"  Low CPU (0-20%): {low} samples ({low/total*100:.1f}%)\")\n",
    "            print(f\"  Medium CPU (20-70%): {medium} samples ({medium/total*100:.1f}%)\")\n",
    "            print(f\"  High CPU (70-100%): {high} samples ({high/total*100:.1f}%)\")\n",
    "        \n",
    "        # Prepare data for LSTM with balanced sampling\n",
    "        sequence_length = 12  # 1 hour at 5-min intervals\n",
    "        X_train, y_train, X_val, y_val, original_y_train, original_y_val, feature_scaler, target_scaler, all_columns = prepare_data_for_lstm(\n",
    "            df, target_col, sequence_length)\n",
    "        \n",
    "        # Create dataloaders with original CPU values for weighted loss\n",
    "        train_dataset = TimeSeriesDataset(X_train, y_train, original_y_train)\n",
    "        val_dataset = TimeSeriesDataset(X_val, y_val, original_y_val)\n",
    "        \n",
    "        batch_size = 64\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "        \n",
    "        # Create enhanced model\n",
    "        input_size = X_train.shape[2]  # Number of features\n",
    "        hidden_size = 64  # More compact model to reduce overfitting\n",
    "        num_layers = 2   # Fewer layers to reduce overfitting\n",
    "        output_size = 1\n",
    "        dropout_rate = 0.3\n",
    "        \n",
    "        model = EnhancedLSTMModel(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            output_size=output_size,\n",
    "            dropout=dropout_rate\n",
    "        ).to(device)\n",
    "        \n",
    "        # Print model info\n",
    "        print(f\"\\nModel architecture:\")\n",
    "        print(f\"  Enhanced LSTM with attention and residual connections\")\n",
    "        print(f\"  Input size: {input_size}\")\n",
    "        print(f\"  Hidden size: {hidden_size}\")\n",
    "        print(f\"  Number of layers: {num_layers}\")\n",
    "        print(f\"  Dropout rate: {dropout_rate}\")\n",
    "        \n",
    "        # Weighted loss function - emphasize low CPU values\n",
    "        criterion = WeightedMSELoss(low_weight=5.0, med_weight=2.0, high_weight=1.0)\n",
    "        \n",
    "        # Optimizer with weight decay to prevent overfitting\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "        \n",
    "        # Learning rate scheduler\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "        \n",
    "        # Train model\n",
    "        print(f\"\\nTraining {service_type.upper()} LSTM model...\")\n",
    "        num_epochs = 100\n",
    "        patience = 15\n",
    "        model, train_losses, val_losses = train_model(\n",
    "            model, train_loader, val_loader, criterion, optimizer, \n",
    "            num_epochs=num_epochs, patience=patience, scheduler=scheduler,\n",
    "            use_weighted_loss=True\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        print(f\"\\nEvaluating {service_type.upper()} LSTM model...\")\n",
    "        predictions, actuals, metrics = evaluate_model(model, val_loader, criterion, target_scaler, use_weighted_loss=True)\n",
    "        \n",
    "        # Test model at different utilization levels\n",
    "        level_results = test_model_at_different_levels(\n",
    "            model, feature_scaler, target_scaler, df, target_col, sequence_length\n",
    "        )\n",
    "        \n",
    "        # Plot results\n",
    "        plot_results(train_losses, val_losses, predictions, actuals, metrics, service_type)\n",
    "        \n",
    "        # Save model and scalers\n",
    "        model_data = {\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'input_size': input_size,\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers,\n",
    "            'sequence_length': sequence_length,\n",
    "            'feature_scaler': feature_scaler,\n",
    "            'target_scaler': target_scaler,\n",
    "            'all_columns': all_columns,\n",
    "            'metrics': metrics,\n",
    "            'level_results': level_results\n",
    "        }\n",
    "        \n",
    "        torch.save(model_data, f'models/{service_type}_lstm_model_enhanced.pth')\n",
    "        print(f\"{service_type.upper()} model saved as models/{service_type}_lstm_model_enhanced.pth\")\n",
    "        \n",
    "        # Additional plot: Training loss vs utilization level performance\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(train_losses, label='Training Loss')\n",
    "        plt.plot(val_losses, label='Validation Loss')\n",
    "        \n",
    "        # Add horizontal lines for MAE by utilization level\n",
    "        plt.axhline(y=metrics['low_mae'], color='blue', linestyle='--', label=f'Low Util. MAE: {metrics[\"low_mae\"]:.2f}')\n",
    "        plt.axhline(y=metrics['med_mae'], color='green', linestyle='--', label=f'Medium Util. MAE: {metrics[\"med_mae\"]:.2f}')\n",
    "        plt.axhline(y=metrics['high_mae'], color='red', linestyle='--', label=f'High Util. MAE: {metrics[\"high_mae\"]:.2f}')\n",
    "        \n",
    "        plt.title(f'{service_type.upper()} - Training Progress and Performance by Utilization Level')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss / MAE')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.savefig(f'plots/{service_type}_training_by_utilization.png')\n",
    "        \n",
    "        return model, (feature_scaler, target_scaler), metrics, level_results\n",
    "    \n",
    "    except Exception as e:\n",
    "        import traceback\n",
    "        print(f\"ERROR processing {service_type}: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, None, None, None\n",
    "\n",
    "# Function to generate synthetic data for under-represented ranges\n",
    "def augment_data_for_low_utilization(df, target_col, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate synthetic data points specifically for low CPU utilization ranges\n",
    "    where the model tends to perform poorly\n",
    "    \"\"\"\n",
    "    print(f\"Augmenting data with {num_samples} synthetic low-utilization samples...\")\n",
    "    \n",
    "    # Copy the original dataframe\n",
    "    df_original = df.copy()\n",
    "    \n",
    "    # Get service type\n",
    "    service_type = 'ec2' if 'EC2' in target_col else 'rds' if 'RDS' in target_col else 'ecs'\n",
    "    \n",
    "    # Create a new dataframe for synthetic data\n",
    "    low_samples = []\n",
    "    \n",
    "    # Get low utilization samples as templates\n",
    "    low_df = df[df[target_col] <= 20].copy()\n",
    "    \n",
    "    if len(low_df) < 10:\n",
    "        print(\"Not enough low utilization samples to use as templates. Using random samples instead.\")\n",
    "        low_df = df.sample(min(100, len(df)))\n",
    "    \n",
    "    # Generate synthetic samples based on low utilization templates\n",
    "    for _ in range(num_samples):\n",
    "        # Pick a random low utilization sample\n",
    "        template = low_df.sample(1)\n",
    "        \n",
    "        # Create a synthetic sample with small random variations\n",
    "        synthetic = template.copy()\n",
    "        \n",
    "        # Set CPU utilization to a random low value (0-20%)\n",
    "        synthetic[target_col] = np.random.uniform(1, 20)\n",
    "        \n",
    "        # Add random noise to other features (but keep within reasonable bounds)\n",
    "        for col in synthetic.columns:\n",
    "            if col != target_col and col != 'timestamp' and not col.startswith('utilization_level'):\n",
    "                # Add up to ±20% noise to numerical columns\n",
    "                if synthetic[col].dtype in [np.int64, np.float64]:\n",
    "                    noise_factor = np.random.uniform(0.8, 1.2)\n",
    "                    synthetic[col] = synthetic[col] * noise_factor\n",
    "        \n",
    "        low_samples.append(synthetic)\n",
    "    \n",
    "    # Concatenate synthetic samples\n",
    "    df_synthetic = pd.concat(low_samples)\n",
    "    \n",
    "    # Reset index\n",
    "    df_synthetic = df_synthetic.reset_index(drop=True)\n",
    "    \n",
    "    # Combine original and synthetic data\n",
    "    df_augmented = pd.concat([df_original, df_synthetic])\n",
    "    \n",
    "    # Reset index\n",
    "    df_augmented = df_augmented.reset_index(drop=True)\n",
    "    \n",
    "    # Print distribution after augmentation\n",
    "    low = len(df_augmented[df_augmented[target_col] <= 20])\n",
    "    medium = len(df_augmented[(df_augmented[target_col] > 20) & (df_augmented[target_col] <= 70)])\n",
    "    high = len(df_augmented[df_augmented[target_col] > 70])\n",
    "    total = len(df_augmented)\n",
    "    \n",
    "    print(f\"Data distribution after augmentation:\")\n",
    "    print(f\"  Low CPU (0-20%): {low} samples ({low/total*100:.1f}%)\")\n",
    "    print(f\"  Medium CPU (20-70%): {medium} samples ({medium/total*100:.1f}%)\")\n",
    "    print(f\"  High CPU (70-100%): {high} samples ({high/total*100:.1f}%)\")\n",
    "    \n",
    "    return df_augmented\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Define services to process\n",
    "    services = [\n",
    "        {'file': '../dataset scripts/ec2_metrics.csv', 'type': 'ec2', 'target': 'EC2_CPUUtilization'},\n",
    "        {'file': '../dataset scripts/rds_metrics.csv', 'type': 'rds', 'target': 'RDS_CPUUtilization'},\n",
    "        {'file': '../dataset scripts/ecs_metrics.csv', 'type': 'ecs', 'target': 'ECS_CPUUtilization'}\n",
    "    ]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for service in services:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {service['type'].upper()} service\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        # Load and preprocess data\n",
    "        df = load_and_preprocess(service['file'])\n",
    "        \n",
    "        # Check if low utilization samples are underrepresented\n",
    "        low = len(df[df[service['target']] <= 20])\n",
    "        total = len(df)\n",
    "        low_percentage = low / total * 100\n",
    "        \n",
    "        # If less than 30% of data is low utilization, augment it\n",
    "        if low_percentage < 30:\n",
    "            print(f\"Low utilization samples are underrepresented ({low_percentage:.1f}%). Augmenting data...\")\n",
    "            # Calculate how many samples to add to reach approximately 33% low utilization\n",
    "            num_samples_to_add = max(0, int((total * 0.5) - low))\n",
    "            df = augment_data_for_low_utilization(df, service['target'], num_samples=num_samples_to_add)\n",
    "        \n",
    "        # Add features\n",
    "        df = add_features(df, service['type'])\n",
    "        \n",
    "        # Run pipeline with augmented data\n",
    "        model, scalers, metrics, level_results = run_lstm_pipeline(\n",
    "            service['file'], service['type'], service['target']\n",
    "        )\n",
    "        \n",
    "        if model is not None:\n",
    "            results[service['type']] = {\n",
    "                'metrics': metrics,\n",
    "                'level_results': level_results\n",
    "            }\n",
    "    \n",
    "    # Print summary of results\n",
    "    print(\"\\n\\n\" + \"=\"*50)\n",
    "    print(\"SUMMARY OF RESULTS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for service_type, result in results.items():\n",
    "        print(f\"\\n{service_type.upper()} Model:\")\n",
    "        print(f\"  Overall - MAE: {result['metrics']['mae']:.4f}, RMSE: {result['metrics']['rmse']:.4f}\")\n",
    "        print(f\"  Low Util. MAE: {result['metrics']['low_mae']:.4f} ({result['metrics']['low_count']} samples)\")\n",
    "        print(f\"  Medium Util. MAE: {result['metrics']['med_mae']:.4f} ({result['metrics']['med_count']} samples)\")\n",
    "        print(f\"  High Util. MAE: {result['metrics']['high_mae']:.4f} ({result['metrics']['high_count']} samples)\")\n",
    "        \n",
    "        for level, level_metrics in result['level_results'].items():\n",
    "            print(f\"  {level.capitalize()} utilization test - Avg Error: {level_metrics['avg_error']:.4f}, Max Error: {level_metrics['max_error']:.4f}\")\n",
    "    \n",
    "    # Create comparative visualization\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Prepare data for bar chart\n",
    "    service_names = list(results.keys())\n",
    "    low_mae = [results[s]['metrics']['low_mae'] for s in service_names]\n",
    "    med_mae = [results[s]['metrics']['med_mae'] for s in service_names]\n",
    "    high_mae = [results[s]['metrics']['high_mae'] for s in service_names]\n",
    "    \n",
    "    x = np.arange(len(service_names))\n",
    "    width = 0.25\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.bar(x - width, low_mae, width, label='Low Utilization', color='blue')\n",
    "    plt.bar(x, med_mae, width, label='Medium Utilization', color='green')\n",
    "    plt.bar(x + width, high_mae, width, label='High Utilization', color='red')\n",
    "    \n",
    "    plt.xlabel('Service')\n",
    "    plt.ylabel('Mean Absolute Error')\n",
    "    plt.title('Model Performance by Utilization Level')\n",
    "    plt.xticks(x, [s.upper() for s in service_names])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Sample distribution\n",
    "    plt.subplot(2, 1, 2)\n",
    "    for i, service_type in enumerate(service_names):\n",
    "        low_count = results[service_type]['metrics']['low_count']\n",
    "        med_count = results[service_type]['metrics']['med_count']\n",
    "        high_count = results[service_type]['metrics']['high_count']\n",
    "        total = low_count + med_count + high_count\n",
    "        \n",
    "        plt.bar(i-width, low_count/total*100, width, label='Low' if i==0 else \"\", color='blue')\n",
    "        plt.bar(i, med_count/total*100, width, label='Medium' if i==0 else \"\", color='green')\n",
    "        plt.bar(i+width, high_count/total*100, width, label='High' if i==0 else \"\", color='red')\n",
    "    \n",
    "    plt.xlabel('Service')\n",
    "    plt.ylabel('Percentage of Samples')\n",
    "    plt.title('Data Distribution by Utilization Level')\n",
    "    plt.xticks(x, [s.upper() for s in service_names])\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('plots/comparative_results.png')\n",
    "    \n",
    "    print(\"\\nAll models trained and evaluated. Results saved to the 'models' and 'plots' directories.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LSTM models for EC2, RDS, and ECS across different usage levels...\n",
      "\n",
      "Testing EC2 model...\n",
      "Loaded ec2 model with input size 22, sequence length 12\n",
      "  Testing low usage scenario...\n",
      "Generating test data for ec2 at low usage level\n",
      "Error testing ec2 model: X has 22 features, but StandardScaler is expecting 21 features as input.\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_93153/2041133592.py\", line 239, in test_models\n",
      "    predicted_cpu = predict_cpu_utilization(model, feature_scaler, target_scaler, df)\n",
      "  File \"/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_93153/2041133592.py\", line 199, in predict_cpu_utilization\n",
      "    df_scaled = feature_scaler.transform(df.values)\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 1062, in transform\n",
      "    X = validate_data(\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2965, in validate_data\n",
      "    _check_n_features(_estimator, X, reset=reset)\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2829, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 22 features, but StandardScaler is expecting 21 features as input.\n",
      "\n",
      "\n",
      "Testing RDS model...\n",
      "Loaded rds model with input size 23, sequence length 12\n",
      "  Testing low usage scenario...\n",
      "Generating test data for rds at low usage level\n",
      "Error testing rds model: X has 23 features, but StandardScaler is expecting 22 features as input.\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_93153/2041133592.py\", line 239, in test_models\n",
      "    predicted_cpu = predict_cpu_utilization(model, feature_scaler, target_scaler, df)\n",
      "  File \"/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_93153/2041133592.py\", line 199, in predict_cpu_utilization\n",
      "    df_scaled = feature_scaler.transform(df.values)\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 1062, in transform\n",
      "    X = validate_data(\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2965, in validate_data\n",
      "    _check_n_features(_estimator, X, reset=reset)\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2829, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 23 features, but StandardScaler is expecting 22 features as input.\n",
      "\n",
      "\n",
      "Testing ECS model...\n",
      "Loaded ecs model with input size 22, sequence length 12\n",
      "  Testing low usage scenario...\n",
      "Generating test data for ecs at low usage level\n",
      "Error testing ecs model: X has 22 features, but StandardScaler is expecting 21 features as input.\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_93153/2041133592.py\", line 239, in test_models\n",
      "    predicted_cpu = predict_cpu_utilization(model, feature_scaler, target_scaler, df)\n",
      "  File \"/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_93153/2041133592.py\", line 199, in predict_cpu_utilization\n",
      "    df_scaled = feature_scaler.transform(df.values)\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/utils/_set_output.py\", line 319, in wrapped\n",
      "    data_to_wrap = f(self, X, *args, **kwargs)\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/preprocessing/_data.py\", line 1062, in transform\n",
      "    X = validate_data(\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2965, in validate_data\n",
      "    _check_n_features(_estimator, X, reset=reset)\n",
      "  File \"/Users/hwimalasooriya/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/sklearn/utils/validation.py\", line 2829, in _check_n_features\n",
      "    raise ValueError(\n",
      "ValueError: X has 22 features, but StandardScaler is expecting 21 features as input.\n",
      "\n",
      "No results to visualize. All models encountered errors.\n",
      "Test results saved to test_results/test_summary.txt\n",
      "Error comparison chart saved to test_results/error_comparison.png\n",
      "Predictions scatter plot saved to test_results/predictions_scatter.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1500x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import traceback\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(\"test_results\", exist_ok=True)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Define the Enhanced LSTM Model with attention and residual connections\n",
    "class EnhancedLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(EnhancedLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        \n",
    "        # Layer normalization\n",
    "        self.layer_norm1 = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size // 2, 1),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "        \n",
    "        # Fully connected layers with residual connection\n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.fc2 = nn.Linear(hidden_size // 2, hidden_size // 2)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, output_size)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Make sure x is 3D [batch_size, sequence_length, input_size]\n",
    "        if len(x.size()) == 2:\n",
    "            x = x.unsqueeze(0)  # Add batch dimension if missing\n",
    "            \n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))  # lstm_out: batch_size, seq_len, hidden_size\n",
    "        \n",
    "        # Apply layer normalization\n",
    "        lstm_out = self.layer_norm1(lstm_out)\n",
    "        \n",
    "        # Attention mechanism\n",
    "        attention_weights = self.attention(lstm_out)\n",
    "        context = torch.sum(attention_weights * lstm_out, dim=1)\n",
    "        \n",
    "        # Fully connected layers with residual connection\n",
    "        out = self.fc1(context)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        residual = out\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = out + residual  # Residual connection\n",
    "        \n",
    "        out = self.fc3(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Load models and scalers\n",
    "def load_model(service_type):\n",
    "    device = torch.device(\"cpu\")  # Use CPU for simplicity\n",
    "    \n",
    "    try:\n",
    "        # Try to load enhanced model first\n",
    "        model_path = f'models/{service_type}_lstm_model_enhanced.pth'\n",
    "        if not os.path.exists(model_path):\n",
    "            # Fall back to regular model\n",
    "            model_path = f'models/{service_type}_lstm_model.pth'\n",
    "            \n",
    "        print(f\"Loading model from {model_path}\")\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # Create model instance\n",
    "        input_size = checkpoint['input_size']\n",
    "        hidden_size = checkpoint['hidden_size']\n",
    "        num_layers = checkpoint['num_layers']\n",
    "        sequence_length = checkpoint['sequence_length']\n",
    "        \n",
    "        model = EnhancedLSTMModel(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            output_size=1,\n",
    "            dropout=0.3\n",
    "        )\n",
    "        \n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        model.eval()\n",
    "        \n",
    "        # Get scalers and column info\n",
    "        feature_scaler = checkpoint['feature_scaler']\n",
    "        target_scaler = checkpoint['target_scaler']\n",
    "        all_columns = checkpoint.get('all_columns', None)\n",
    "        \n",
    "        print(f\"Loaded {service_type} model with input size {input_size}, hidden size {hidden_size}, sequence length {sequence_length}\")\n",
    "        \n",
    "        return model, feature_scaler, target_scaler, sequence_length, all_columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None, None, None, None, None\n",
    "\n",
    "# Generate synthetic test data for different resource usage levels\n",
    "def generate_test_data(service_type, usage_level, sequence_length, all_columns=None):\n",
    "    \"\"\"Generate synthetic test data with consistent features across all models\"\"\"\n",
    "    print(f\"Generating test data for {service_type} at {usage_level} usage level\")\n",
    "    \n",
    "    # Set base CPU level based on usage scenario\n",
    "    if usage_level == \"low\":\n",
    "        base_cpu = np.random.uniform(5, 15)  # 5-15% CPU usage\n",
    "    elif usage_level == \"medium\":\n",
    "        base_cpu = np.random.uniform(40, 60)  # 40-60% CPU usage\n",
    "    else:  # high\n",
    "        base_cpu = np.random.uniform(80, 95)  # 80-95% CPU usage\n",
    "    \n",
    "    # Create timestamps for the sequence\n",
    "    end_time = datetime.now()\n",
    "    timestamps = [end_time - timedelta(minutes=5*i) for i in range(sequence_length)]\n",
    "    timestamps.reverse()  # Sort in ascending order\n",
    "    \n",
    "    # Initialize DataFrame with timestamps\n",
    "    df = pd.DataFrame(index=timestamps)\n",
    "    df.index.name = 'timestamp'\n",
    "    \n",
    "    # Get the CPU column name\n",
    "    if service_type == 'ec2':\n",
    "        cpu_col = 'EC2_CPUUtilization'\n",
    "        memory_col = 'EC2_MemoryUtilization'\n",
    "        disk_col = 'EC2_DiskWriteOps'\n",
    "        network_col = 'EC2_NetworkIn'\n",
    "    elif service_type == 'rds':\n",
    "        cpu_col = 'RDS_CPUUtilization'\n",
    "        memory_col = 'RDS_FreeableMemory'\n",
    "        conn_col = 'RDS_DatabaseConnections'\n",
    "        io_col = 'RDS_WriteIOPS'\n",
    "    else:  # ecs\n",
    "        cpu_col = 'ECS_CPUUtilization'\n",
    "        memory_col = 'ECS_MemoryUtilization'\n",
    "        task_col = 'ECS_RunningTaskCount'\n",
    "        network_col = 'ECS_NetworkIn'\n",
    "    \n",
    "    # Generate base metrics with slight variations\n",
    "    # Create a smooth curve with some noise\n",
    "    time_points = np.linspace(0, 2*np.pi, sequence_length)\n",
    "    noise = np.random.normal(0, 2, sequence_length)\n",
    "    \n",
    "    if usage_level == \"low\":\n",
    "        # Keep low utilization but add some small variations\n",
    "        cpu_values = base_cpu + 5 * np.sin(time_points) + noise\n",
    "    elif usage_level == \"medium\":\n",
    "        # Medium utilization with moderate variations\n",
    "        cpu_values = base_cpu + 10 * np.sin(time_points) + noise\n",
    "    else:  # high\n",
    "        # High utilization with some variations\n",
    "        cpu_values = base_cpu + 5 * np.sin(time_points) + noise\n",
    "    \n",
    "    # Ensure values stay within 0-100%\n",
    "    cpu_values = np.clip(cpu_values, 0, 100)\n",
    "    df[cpu_col] = cpu_values\n",
    "    \n",
    "    # Add service-specific features\n",
    "    if service_type == 'ec2':\n",
    "        # Memory usually correlates with CPU but with some variance\n",
    "        memory_factor = 0.8 + np.random.rand() * 0.4  # 0.8 to 1.2\n",
    "        df[memory_col] = df[cpu_col] * memory_factor\n",
    "        df[memory_col] = np.clip(df[memory_col], 0, 100)\n",
    "        \n",
    "        # Disk and network vary more\n",
    "        df[disk_col] = df[cpu_col] * (0.5 + np.random.rand()) + np.random.normal(0, 100, len(df))\n",
    "        df[disk_col] = np.maximum(0, df[disk_col])  # No negative I/O\n",
    "        \n",
    "        df[network_col] = df[cpu_col] * (0.3 + np.random.rand()) + np.random.normal(0, 200, len(df))\n",
    "        df[network_col] = np.maximum(0, df[network_col])  # No negative network\n",
    "        \n",
    "    elif service_type == 'rds':\n",
    "        # RDS FreeableMemory is inversely related to CPU\n",
    "        mem_base = 10000 - (df[cpu_col] * 50)\n",
    "        df[memory_col] = mem_base + np.random.normal(0, 500, len(df))\n",
    "        df[memory_col] = np.maximum(100, df[memory_col])  # Ensure positive\n",
    "        \n",
    "        # Connections often correlate with CPU\n",
    "        df[conn_col] = df[cpu_col] * 5 + np.random.normal(0, 20, len(df))\n",
    "        df[conn_col] = np.maximum(1, df[conn_col])  # At least 1 connection\n",
    "        \n",
    "        # IO can be spiky\n",
    "        df[io_col] = df[cpu_col] * 10 + np.random.exponential(100, len(df))\n",
    "        df[io_col] = np.maximum(0, df[io_col])\n",
    "        \n",
    "    else:  # ecs\n",
    "        # Memory often correlates strongly with CPU in containers\n",
    "        df[memory_col] = df[cpu_col] * 1.2 + np.random.normal(0, 5, len(df))\n",
    "        df[memory_col] = np.clip(df[memory_col], 0, 100)\n",
    "        \n",
    "        # Task count is discrete and generally correlates with load\n",
    "        base_tasks = np.maximum(1, df[cpu_col] / 20).astype(int)\n",
    "        df[task_col] = base_tasks + np.random.randint(0, 2, len(df))\n",
    "        \n",
    "        # Network traffic\n",
    "        df[network_col] = df[cpu_col] * 15 + np.random.normal(0, 100, len(df))\n",
    "        df[network_col] = np.maximum(0, df[network_col])\n",
    "    \n",
    "    # Add time-based features\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['is_weekend'] = df['day_of_week'].apply(lambda x: 1 if x >= 5 else 0)\n",
    "    df['is_business_hours'] = ((df['hour'] >= 9) & (df['hour'] < 17) & (df['is_weekend'] == 0)).astype(int)\n",
    "    \n",
    "    # Create utilization level categorical feature\n",
    "    def get_utilization_level(cpu):\n",
    "        if cpu <= 20:\n",
    "            return 0  # low\n",
    "        elif cpu <= 70:\n",
    "            return 1  # medium\n",
    "        else:\n",
    "            return 2  # high\n",
    "    \n",
    "    df['utilization_level'] = df[cpu_col].apply(get_utilization_level)\n",
    "    df['is_low_utilization'] = (df['utilization_level'] == 0).astype(int)\n",
    "    df['is_medium_utilization'] = (df['utilization_level'] == 1).astype(int)\n",
    "    df['is_high_utilization'] = (df['utilization_level'] == 2).astype(int)\n",
    "    \n",
    "    # Add lag features\n",
    "    for i in range(1, 4):\n",
    "        df[f'{cpu_col}_lag_{i}'] = df[cpu_col].shift(i)\n",
    "    \n",
    "    # Add differencing features\n",
    "    df[f'{cpu_col}_diff_1'] = df[cpu_col].diff(1)\n",
    "    df[f'{cpu_col}_diff_3'] = df[cpu_col].diff(3)\n",
    "    \n",
    "    # Add rolling statistics\n",
    "    df[f'{cpu_col}_roll_mean_30m'] = df[cpu_col].rolling(min_periods=1, window=6).mean()\n",
    "    df[f'{cpu_col}_roll_mean_1h'] = df[cpu_col].rolling(min_periods=1, window=12).mean()\n",
    "    df[f'{cpu_col}_roll_std_1h'] = df[cpu_col].rolling(min_periods=1, window=12).std().fillna(0)\n",
    "    \n",
    "    # Add rate of change features\n",
    "    df[f'{cpu_col}_roc_5m'] = df[cpu_col].pct_change(periods=1).fillna(0)\n",
    "    df[f'{cpu_col}_roc_15m'] = df[cpu_col].pct_change(periods=3).fillna(0)\n",
    "    \n",
    "    # Cyclic encoding of time\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    # Service-specific ratios\n",
    "    if service_type == 'ec2':\n",
    "        df['EC2_CPU_Memory_Ratio'] = df[cpu_col] / (df[memory_col] + 1e-8)\n",
    "        df['EC2_CPU_Network_Ratio'] = df[cpu_col] / (df[network_col] + 1e-8)\n",
    "        df['EC2_CPU_Disk_Ratio'] = df[cpu_col] / (df[disk_col] + 1e-8)\n",
    "        df['EC2_is_high_memory'] = (df[memory_col] > 70).astype(int)\n",
    "        df['EC2_memory_to_cpu_diff'] = df[memory_col] - df[cpu_col]\n",
    "        \n",
    "    elif service_type == 'rds':\n",
    "        df['RDS_Memory_Usage_Estimate'] = 100 - (df[memory_col] / df[memory_col].max() * 100)\n",
    "        df['RDS_CPU_Memory_Ratio'] = df[cpu_col] / (df['RDS_Memory_Usage_Estimate'] + 1e-8)\n",
    "        df['RDS_CPU_Conn_Ratio'] = df[cpu_col] / (df[conn_col] + 1e-8)\n",
    "        df['RDS_CPU_IO_Ratio'] = df[cpu_col] / (df[io_col] + 1e-8)\n",
    "        df['RDS_conn_per_cpu'] = df[conn_col] / (df[cpu_col] + 1e-8)\n",
    "        df['RDS_io_intensive'] = (df[io_col] > df[io_col].mean() * 1.5).astype(int)\n",
    "        \n",
    "    elif service_type == 'ecs':\n",
    "        df['ECS_CPU_Memory_Ratio'] = df[cpu_col] / (df[memory_col] + 1e-8)\n",
    "        df['ECS_CPU_Task_Ratio'] = df[cpu_col] / (df[task_col] + 1e-8)\n",
    "        df['ECS_CPU_Network_Ratio'] = df[cpu_col] / (df[network_col] + 1e-8)\n",
    "        df['ECS_per_task_cpu'] = df[cpu_col] / (df[task_col] + 1e-8)\n",
    "        df['ECS_is_memory_bound'] = (df[memory_col] > df[cpu_col]).astype(int)\n",
    "    \n",
    "    # Fill missing values\n",
    "    df = df.fillna(0)\n",
    "    \n",
    "    # Extract actual CPU value (last value in sequence)\n",
    "    actual_cpu = df[cpu_col].iloc[-1]\n",
    "    \n",
    "    # If specific columns are provided, adjust DataFrame to match\n",
    "    if all_columns is not None:\n",
    "        # Check for missing columns\n",
    "        for col in all_columns:\n",
    "            if col not in df.columns:\n",
    "                print(f\"Adding missing column: {col}\")\n",
    "                df[col] = 0  # Fill with zeros if missing\n",
    "        \n",
    "        # Ensure columns are in the right order\n",
    "        df = df[all_columns]\n",
    "    \n",
    "    return df, actual_cpu\n",
    "\n",
    "# Make a prediction\n",
    "def predict_cpu_utilization(model, feature_scaler, target_scaler, df):\n",
    "    \"\"\"Make a CPU utilization prediction using the loaded model\"\"\"\n",
    "    try:\n",
    "        # Scale the features\n",
    "        df_scaled = feature_scaler.transform(df.values)\n",
    "        \n",
    "        # Convert to torch tensor with batch dimension\n",
    "        input_tensor = torch.tensor(df_scaled, dtype=torch.float32).unsqueeze(0)\n",
    "        \n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            scaled_prediction = model(input_tensor).numpy()\n",
    "        \n",
    "        # Inverse transform the prediction\n",
    "        prediction = target_scaler.inverse_transform(scaled_prediction)[0, 0]\n",
    "        \n",
    "        # Ensure prediction is within valid range (0-100%)\n",
    "        prediction = max(0, min(100, prediction))\n",
    "        \n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        print(f\"Error making prediction: {str(e)}\")\n",
    "        print(traceback.format_exc())\n",
    "        return None\n",
    "\n",
    "# Test function to run tests for all services at all usage levels\n",
    "def test_models():\n",
    "    \"\"\"Test all models at different usage levels with consistent feature sets\"\"\"\n",
    "    services = ['ec2', 'rds', 'ecs']\n",
    "    usage_levels = ['low', 'medium', 'high']\n",
    "    num_samples = 10  # Number of samples to test for each level\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for service in services:\n",
    "        try:\n",
    "            print(f\"\\nTesting {service.upper()} model...\")\n",
    "            model, feature_scaler, target_scaler, sequence_length, all_columns = load_model(service)\n",
    "            \n",
    "            if model is None:\n",
    "                print(f\"Skipping {service} - model could not be loaded\")\n",
    "                continue\n",
    "            \n",
    "            service_results = {}\n",
    "            for level in usage_levels:\n",
    "                print(f\"  Testing {level} usage scenario...\")\n",
    "                \n",
    "                # Test with multiple samples for this level\n",
    "                level_results = []\n",
    "                for i in range(num_samples):\n",
    "                    # Generate test data with consistent features\n",
    "                    df, actual_cpu = generate_test_data(service, level, sequence_length, all_columns)\n",
    "                    \n",
    "                    # Make prediction\n",
    "                    predicted_cpu = predict_cpu_utilization(model, feature_scaler, target_scaler, df)\n",
    "                    \n",
    "                    if predicted_cpu is not None:\n",
    "                        error = abs(predicted_cpu - actual_cpu)\n",
    "                        error_percent = (error / max(actual_cpu, 1e-8)) * 100\n",
    "                        \n",
    "                        level_results.append({\n",
    "                            'sample': i+1,\n",
    "                            'actual_cpu': float(actual_cpu),\n",
    "                            'predicted_cpu': float(predicted_cpu),\n",
    "                            'absolute_error': float(error),\n",
    "                            'error_percent': float(error_percent)\n",
    "                        })\n",
    "                        \n",
    "                        print(f\"    Sample {i+1}: Actual={actual_cpu:.2f}%, Predicted={predicted_cpu:.2f}%, Error={error:.2f}% ({error_percent:.2f}%)\")\n",
    "                \n",
    "                # Calculate summary statistics for this level\n",
    "                if level_results:\n",
    "                    avg_error = np.mean([r['absolute_error'] for r in level_results])\n",
    "                    max_error = np.max([r['absolute_error'] for r in level_results])\n",
    "                    avg_error_percent = np.mean([r['error_percent'] for r in level_results])\n",
    "                    max_error_percent = np.max([r['error_percent'] for r in level_results])\n",
    "                    \n",
    "                    level_summary = {\n",
    "                        'samples': level_results,\n",
    "                        'avg_error': float(avg_error),\n",
    "                        'max_error': float(max_error),\n",
    "                        'avg_error_percent': float(avg_error_percent),\n",
    "                        'max_error_percent': float(max_error_percent)\n",
    "                    }\n",
    "                    \n",
    "                    service_results[level] = level_summary\n",
    "            \n",
    "            # Calculate overall performance for this service\n",
    "            all_errors = []\n",
    "            for level in service_results:\n",
    "                all_errors.extend([s['absolute_error'] for s in service_results[level]['samples']])\n",
    "            \n",
    "            overall_avg_error = np.mean(all_errors) if all_errors else 0\n",
    "            \n",
    "            service_results['overall'] = {\n",
    "                'avg_error': float(overall_avg_error)\n",
    "            }\n",
    "            \n",
    "            results[service] = service_results\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error testing {service} model: {str(e)}\")\n",
    "            print(traceback.format_exc())\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Visualize results\n",
    "def visualize_results(results):\n",
    "    \"\"\"Create detailed visualizations of model performance\"\"\"\n",
    "    if not results:\n",
    "        print(\"No results to visualize. All models encountered errors.\")\n",
    "        return\n",
    "    \n",
    "    services = list(results.keys())\n",
    "    usage_levels = ['low', 'medium', 'high']\n",
    "    \n",
    "    # 1. Prediction Accuracy Plot\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, service in enumerate(services):\n",
    "        plt.subplot(len(services), 1, i+1)\n",
    "        \n",
    "        for level in usage_levels:\n",
    "            if level not in results[service]:\n",
    "                continue\n",
    "                \n",
    "            actuals = [r['actual_cpu'] for r in results[service][level]['samples']]\n",
    "            predictions = [r['predicted_cpu'] for r in results[service][level]['samples']]\n",
    "            \n",
    "            # Sort by actual values for better visualization\n",
    "            sorted_indices = np.argsort(actuals)\n",
    "            sorted_actuals = [actuals[idx] for idx in sorted_indices]\n",
    "            sorted_predictions = [predictions[idx] for idx in sorted_indices]\n",
    "            \n",
    "            plt.scatter(range(len(sorted_actuals)), sorted_actuals, label=f'{level} Actual', marker='o')\n",
    "            plt.scatter(range(len(sorted_predictions)), sorted_predictions, label=f'{level} Predicted', marker='x')\n",
    "            \n",
    "            # Add connecting lines between actual and predicted points\n",
    "            for j in range(len(sorted_actuals)):\n",
    "                plt.plot([j, j], [sorted_actuals[j], sorted_predictions[j]], 'k-', alpha=0.3)\n",
    "        \n",
    "        plt.title(f'{service.upper()} Model Predictions')\n",
    "        plt.xlabel('Sample (sorted by actual CPU)')\n",
    "        plt.ylabel('CPU Utilization (%)')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.ylim(0, 100)  # Ensure y-axis shows full range\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_results/model_predictions.png')\n",
    "    \n",
    "    # 2. Error Analysis - Bar Chart\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Prepare data for bar chart\n",
    "    service_names = []\n",
    "    low_errors = []\n",
    "    med_errors = []\n",
    "    high_errors = []\n",
    "    \n",
    "    for service in services:\n",
    "        service_names.append(service.upper())\n",
    "        \n",
    "        for level in usage_levels:\n",
    "            if level in results[service]:\n",
    "                avg_error = results[service][level]['avg_error']\n",
    "                \n",
    "                if level == 'low':\n",
    "                    low_errors.append(avg_error)\n",
    "                elif level == 'medium':\n",
    "                    med_errors.append(avg_error)\n",
    "                else:\n",
    "                    high_errors.append(avg_error)\n",
    "            else:\n",
    "                if level == 'low':\n",
    "                    low_errors.append(0)\n",
    "                elif level == 'medium':\n",
    "                    med_errors.append(0)\n",
    "                else:\n",
    "                    high_errors.append(0)\n",
    "    \n",
    "    # Set width of bars\n",
    "    bar_width = 0.25\n",
    "    x = np.arange(len(service_names))\n",
    "    \n",
    "    # Create bars\n",
    "    plt.bar(x - bar_width, low_errors, bar_width, label='Low Usage (0-20%)', color='skyblue')\n",
    "    plt.bar(x, med_errors, bar_width, label='Medium Usage (40-60%)', color='royalblue')\n",
    "    plt.bar(x + bar_width, high_errors, bar_width, label='High Usage (80-100%)', color='darkblue')\n",
    "    \n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Service')\n",
    "    plt.ylabel('Mean Absolute Error (%)')\n",
    "    plt.title('Model Error by Service and Usage Level')\n",
    "    plt.xticks(x, service_names)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add a horizontal line at 10% error (good threshold)\n",
    "    plt.axhline(y=10, color='r', linestyle='--', alpha=0.5, label='10% Error Threshold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_results/error_comparison.png')\n",
    "    \n",
    "    # 3. Scatter Plot: Actual vs Predicted\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    for i, service in enumerate(services):\n",
    "        plt.subplot(1, len(services), i+1)\n",
    "        \n",
    "        all_actuals = []\n",
    "        all_predictions = []\n",
    "        all_colors = []\n",
    "        \n",
    "        color_map = {'low': 'blue', 'medium': 'green', 'high': 'red'}\n",
    "        \n",
    "        for level in usage_levels:\n",
    "            if level in results[service]:\n",
    "                actuals = [r['actual_cpu'] for r in results[service][level]['samples']]\n",
    "                predictions = [r['predicted_cpu'] for r in results[service][level]['samples']]\n",
    "                \n",
    "                all_actuals.extend(actuals)\n",
    "                all_predictions.extend(predictions)\n",
    "                all_colors.extend([color_map[level]] * len(actuals))\n",
    "        \n",
    "        # Plot scatter with colors by level\n",
    "        plt.scatter(all_actuals, all_predictions, c=all_colors, alpha=0.7)\n",
    "        \n",
    "        # Add perfect prediction line\n",
    "        plt.plot([0, 100], [0, 100], 'k--', alpha=0.5)\n",
    "        \n",
    "        # Add +/- 10% error bands\n",
    "        plt.plot([0, 100], [0, 110], 'r--', alpha=0.3)\n",
    "        plt.plot([0, 100], [0, 90], 'r--', alpha=0.3)\n",
    "        \n",
    "        plt.title(f'{service.upper()} - Predicted vs Actual')\n",
    "        plt.xlabel('Actual CPU Utilization (%)')\n",
    "        plt.ylabel('Predicted CPU Utilization (%)')\n",
    "        plt.xlim(0, 100)\n",
    "        plt.ylim(0, 100)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add correlation coefficient\n",
    "        correlation = np.corrcoef(all_actuals, all_predictions)[0, 1]\n",
    "        plt.annotate(f'r = {correlation:.2f}', xy=(0.05, 0.95), xycoords='axes fraction',\n",
    "                    fontsize=10, bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('test_results/prediction_scatter.png')\n",
    "    \n",
    "    # Save detailed results to JSON\n",
    "    with open('test_results/detailed_results.json', 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    # Create summary report\n",
    "    with open('test_results/test_summary.txt', 'w') as f:\n",
    "        f.write(\"LSTM Model Test Results\\n\")\n",
    "        f.write(\"======================\\n\\n\")\n",
    "        \n",
    "        for service in services:\n",
    "            f.write(f\"{service.upper()} Model:\\n\")\n",
    "            \n",
    "            # Overall performance\n",
    "            if 'overall' in results[service]:\n",
    "                f.write(f\"  Overall Average Error: {results[service]['overall']['avg_error']:.2f}%\\n\")\n",
    "            \n",
    "            # Performance by level\n",
    "            for level in usage_levels:\n",
    "                if level in results[service]:\n",
    "                    f.write(f\"  {level.capitalize()} usage - Avg Error: {results[service][level]['avg_error']:.2f}%, \")\n",
    "                    f.write(f\"Max Error: {results[service][level]['max_error']:.2f}%\\n\")\n",
    "                    \n",
    "                    # Sample details (first 3 samples only to keep it readable)\n",
    "                    f.write(\"    Sample details (first 3):\\n\")\n",
    "                    for idx, r in enumerate(results[service][level]['samples'][:3]):\n",
    "                        f.write(f\"      {idx+1}. Actual: {r['actual_cpu']:.2f}%, Predicted: {r['predicted_cpu']:.2f}%, \")\n",
    "                        f.write(f\"Error: {r['absolute_error']:.2f}%\\n\")\n",
    "                    \n",
    "                    f.write(\"\\n\")\n",
    "            \n",
    "            f.write(\"\\n\")\n",
    "        \n",
    "        # Overall conclusion\n",
    "        f.write(\"\\nConclusion:\\n\")\n",
    "        f.write(\"The models have been tested across low, medium, and high CPU utilization scenarios.\\n\")\n",
    "        \n",
    "        # Calculate overall average error across all models and levels\n",
    "        all_errors = []\n",
    "        for service in services:\n",
    "            for level in usage_levels:\n",
    "                if level in results[service]:\n",
    "                    all_errors.extend([s['absolute_error'] for s in results[service][level]['samples']])\n",
    "        \n",
    "        if all_errors:\n",
    "            overall_avg = np.mean(all_errors)\n",
    "            f.write(f\"Overall average error across all tests: {overall_avg:.2f}%\\n\")\n",
    "            \n",
    "            # Provide assessment based on error rates\n",
    "            if overall_avg < 7:\n",
    "                f.write(\"Assessment: Models show excellent performance across all utilization levels.\\n\")\n",
    "            elif overall_avg < 12:\n",
    "                f.write(\"Assessment: Models show good performance overall, with possible room for improvement at specific utilization levels.\\n\")\n",
    "            elif overall_avg < 20:\n",
    "                f.write(\"Assessment: Models show acceptable performance but would benefit from further refinement.\\n\")\n",
    "            else:\n",
    "                f.write(\"Assessment: Models require significant improvement before production deployment.\\n\")\n",
    "    \n",
    "    print(f\"Test results saved to test_results/ directory\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Testing enhanced LSTM models for EC2, RDS, and ECS across different usage levels...\")\n",
    "    results = test_models()\n",
    "    visualize_results(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
