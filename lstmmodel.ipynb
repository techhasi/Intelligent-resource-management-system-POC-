{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create New Columns for AWS Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['timestamp', 'EC2_CPUUtilization', 'EC2_MemoryUtilization',\n",
      "       'EC2_DiskReadOps', 'EC2_DiskWriteOps', 'EC2_NetworkIn',\n",
      "       'EC2_NetworkOut', 'RDS_CPUUtilization', 'RDS_FreeableMemory',\n",
      "       'RDS_DatabaseConnections', 'RDS_ReadIOPS', 'RDS_WriteIOPS',\n",
      "       'ECS_CPUUtilization', 'ECS_MemoryUtilization', 'ECS_RunningTaskCount'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('cloud-computing-performance-metrics.csv')\n",
    "\n",
    "# 1. EC2 Metrics\n",
    "df['EC2_CPUUtilization'] = df['cpu_usage']\n",
    "df['EC2_MemoryUtilization'] = df['memory_usage']\n",
    "df['EC2_DiskReadOps'] = df['num_executed_instructions']  # Simulated\n",
    "df['EC2_DiskWriteOps'] = df['num_executed_instructions']  # Simulated\n",
    "df['EC2_NetworkIn'] = df['network_traffic']  # Simulated\n",
    "df['EC2_NetworkOut'] = df['network_traffic']  # Simulated\n",
    "\n",
    "# 2. RDS Metrics\n",
    "df['RDS_CPUUtilization'] = df['cpu_usage']\n",
    "df['RDS_FreeableMemory'] = 100 - df['memory_usage']  # Simulated freeable memory\n",
    "df['RDS_DatabaseConnections'] = df['task_status'].apply(lambda x: 1 if x == 'running' else 0)  # Simulated\n",
    "df['RDS_ReadIOPS'] = df['num_executed_instructions']  # Simulated\n",
    "df['RDS_WriteIOPS'] = df['num_executed_instructions']  # Simulated\n",
    "\n",
    "# 3. ECS Metrics\n",
    "df['ECS_CPUUtilization'] = df['cpu_usage']\n",
    "df['ECS_MemoryUtilization'] = df['memory_usage']\n",
    "df['ECS_RunningTaskCount'] = df['task_status'].apply(lambda x: 1 if x == 'running' else 0)  # Simulated\n",
    "\n",
    "# List of unwanted columns\n",
    "unwanted_columns = [\n",
    "    'vm_id', 'power_consumption', 'num_executed_instructions', \n",
    "    'execution_time', 'energy_efficiency', 'task_type', \n",
    "    'task_priority', 'task_status', 'cpu_usage', 'memory_usage', 'network_traffic'\n",
    "]\n",
    "\n",
    "# Drop unwanted columns\n",
    "df.drop(columns=unwanted_columns, inplace=True)\n",
    "\n",
    "# Display the remaining columns\n",
    "print(df.columns)\n",
    "\n",
    "# Save the mapped dataset\n",
    "df.to_csv('mapped_cloud_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Mapped Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp  EC2_CPUUtilization  EC2_MemoryUtilization  \\\n",
      "0  2023-01-25 09:10:54           54.881350              78.950861   \n",
      "1  2023-01-26 04:46:34           71.518937              29.901883   \n",
      "2  2023-01-13 23:39:47                 NaN              92.709195   \n",
      "3  2023-02-09 11:45:49           54.488318              88.100960   \n",
      "4  2023-06-14 08:27:26           42.365480                    NaN   \n",
      "\n",
      "   EC2_DiskReadOps  EC2_DiskWriteOps  EC2_NetworkIn  EC2_NetworkOut  \\\n",
      "0           7527.0            7527.0     164.775973      164.775973   \n",
      "1           5348.0            5348.0            NaN             NaN   \n",
      "2           5483.0            5483.0     203.674847      203.674847   \n",
      "3           5876.0            5876.0            NaN             NaN   \n",
      "4           3361.0            3361.0            NaN             NaN   \n",
      "\n",
      "   RDS_CPUUtilization  RDS_FreeableMemory  RDS_DatabaseConnections  \\\n",
      "0           54.881350           21.049139                        0   \n",
      "1           71.518937           70.098117                        0   \n",
      "2                 NaN            7.290805                        0   \n",
      "3           54.488318           11.899040                        0   \n",
      "4           42.365480                 NaN                        0   \n",
      "\n",
      "   RDS_ReadIOPS  RDS_WriteIOPS  ECS_CPUUtilization  ECS_MemoryUtilization  \\\n",
      "0        7527.0         7527.0           54.881350              78.950861   \n",
      "1        5348.0         5348.0           71.518937              29.901883   \n",
      "2        5483.0         5483.0                 NaN              92.709195   \n",
      "3        5876.0         5876.0           54.488318              88.100960   \n",
      "4        3361.0         3361.0           42.365480                    NaN   \n",
      "\n",
      "   ECS_RunningTaskCount  \n",
      "0                     0  \n",
      "1                     0  \n",
      "2                     0  \n",
      "3                     0  \n",
      "4                     0  \n"
     ]
    }
   ],
   "source": [
    "# Load the mapped dataset\n",
    "df = pd.read_csv('mapped_cloud_metrics.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp                  200666\n",
      "EC2_CPUUtilization         199038\n",
      "EC2_MemoryUtilization      200510\n",
      "EC2_DiskReadOps            199686\n",
      "EC2_DiskWriteOps           199686\n",
      "EC2_NetworkIn              199481\n",
      "EC2_NetworkOut             199481\n",
      "RDS_CPUUtilization         199038\n",
      "RDS_FreeableMemory         200510\n",
      "RDS_DatabaseConnections         0\n",
      "RDS_ReadIOPS               199686\n",
      "RDS_WriteIOPS              199686\n",
      "ECS_CPUUtilization         199038\n",
      "ECS_MemoryUtilization      200510\n",
      "ECS_RunningTaskCount            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Step 4: Handle missing values\n",
    "print(df.isnull().sum())\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Step 5: Save the cleaned dataset\n",
    "df.to_csv('cleaned_cloud_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "1. Temporal features\n",
    "2. Rolling Averages\n",
    "3. Lagged features\n",
    "4. Utilization ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_15352/3284802844.py:20: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  df.fillna(method='bfill', inplace=True)  # Backward fill\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            timestamp  EC2_CPUUtilization  EC2_MemoryUtilization  \\\n",
      "0 2023-01-25 09:10:54           54.881350              78.950861   \n",
      "1 2023-01-26 04:46:34           71.518937              29.901883   \n",
      "2 2023-01-13 23:39:47           50.012232              92.709195   \n",
      "3 2023-02-09 11:45:49           54.488318              88.100960   \n",
      "4 2023-06-14 08:27:26           42.365480              49.981898   \n",
      "\n",
      "   EC2_DiskReadOps  EC2_DiskWriteOps  EC2_NetworkIn  EC2_NetworkOut  \\\n",
      "0           7527.0            7527.0     164.775973      164.775973   \n",
      "1           5348.0            5348.0     500.007572      500.007572   \n",
      "2           5483.0            5483.0     203.674847      203.674847   \n",
      "3           5876.0            5876.0     500.007572      500.007572   \n",
      "4           3361.0            3361.0     500.007572      500.007572   \n",
      "\n",
      "   RDS_CPUUtilization  RDS_FreeableMemory  RDS_DatabaseConnections  ...  \\\n",
      "0           54.881350           21.049139                        0  ...   \n",
      "1           71.518937           70.098117                        0  ...   \n",
      "2           50.012232            7.290805                        0  ...   \n",
      "3           54.488318           11.899040                        0  ...   \n",
      "4           42.365480           50.018102                        0  ...   \n",
      "\n",
      "   ECS_RunningTaskCount  hour  day_of_week  month  cpu_rolling_avg  \\\n",
      "0                     0     9            2      1        54.653263   \n",
      "1                     0     4            3      1        54.653263   \n",
      "2                     0    23            4      1        54.653263   \n",
      "3                     0    11            3      2        54.653263   \n",
      "4                     0     8            2      6        54.653263   \n",
      "\n",
      "   memory_rolling_avg  cpu_lag_1  cpu_lag_2  cpu_memory_ratio  \\\n",
      "0           67.928959  54.881350  54.881350          0.695133   \n",
      "1           67.928959  54.881350  54.881350          2.391787   \n",
      "2           67.928959  71.518937  54.881350          0.539453   \n",
      "3           67.928959  50.012232  71.518937          0.618476   \n",
      "4           67.928959  54.488318  50.012232          0.847616   \n",
      "\n",
      "   network_in_out_ratio  \n",
      "0                   1.0  \n",
      "1                   1.0  \n",
      "2                   1.0  \n",
      "3                   1.0  \n",
      "4                   1.0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('cleaned_cloud_metrics.csv')\n",
    "\n",
    "# Ensure timestamp is in datetime format\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 1. Temporal Features\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "# 2. Rolling Averages (e.g., 5-time-step rolling average for CPU and memory)\n",
    "window_size = 5\n",
    "df['cpu_rolling_avg'] = df['EC2_CPUUtilization'].rolling(window=window_size).mean()\n",
    "df['memory_rolling_avg'] = df['EC2_MemoryUtilization'].rolling(window=window_size).mean()\n",
    "\n",
    "# Fill NaN values in rolling averages (first few rows)\n",
    "df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "\n",
    "# 3. Lagged Features (e.g., CPU usage at t-1, t-2)\n",
    "df['cpu_lag_1'] = df['EC2_CPUUtilization'].shift(1)\n",
    "df['cpu_lag_2'] = df['EC2_CPUUtilization'].shift(2)\n",
    "\n",
    "# Fill NaN values in lagged features\n",
    "df.bfill(inplace=True)  # Backward fill\n",
    "\n",
    "# 4. Utilization Ratios\n",
    "df['cpu_memory_ratio'] = df['EC2_CPUUtilization'] / df['EC2_MemoryUtilization']\n",
    "df['network_in_out_ratio'] = df['EC2_NetworkIn'] / df['EC2_NetworkOut']\n",
    "\n",
    "# Handle division by zero (replace infinite values with NaN and then fill with 0)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Display the new features\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1400000\n",
      "Validation set size: 300000\n",
      "Testing set size: 300000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sort by timestamp (ensure data is in chronological order)\n",
    "df.sort_values('timestamp', inplace=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# For LSTM: Target could be future CPU usage (e.g., next time step)\n",
    "X = df.drop(columns=['timestamp'])  # Drop timestamp (not a feature)\n",
    "y = df['EC2_CPUUtilization']  # Example target (can be adjusted)\n",
    "\n",
    "# Time-based split (e.g., 70% train, 15% validation, 15% test)\n",
    "train_size = int(0.7 * len(df))\n",
    "val_size = int(0.15 * len(df))\n",
    "\n",
    "X_train, X_val_test = X[:train_size], X[train_size:]\n",
    "y_train, y_val_test = y[:train_size], y[train_size:]\n",
    "\n",
    "X_val, X_test = X_val_test[:val_size], X_val_test[val_size:]\n",
    "y_val, y_test = y_val_test[:val_size], y_val_test[val_size:]\n",
    "\n",
    "# Verify the splits\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Normalize the target (if needed)\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = scaler.transform(y_val.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training sequences: (1399990, 10, 23), Targets: (1399990, 1)\n",
      "Validation sequences: (299990, 10, 23), Targets: (299990, 1)\n",
      "Testing sequences: (299990, 10, 23), Targets: (299990, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, targets, time_steps=10):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X_seq.append(data[i:i+time_steps])\n",
    "        y_seq.append(targets[i+time_steps])  # Predict the next CPU usage\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Define time steps (e.g., 10 time steps per sequence)\n",
    "time_steps = 10\n",
    "\n",
    "# Create sequences for training, validation, and testing\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, time_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Training sequences: {X_train_seq.shape}, Targets: {y_train_seq.shape}\")\n",
    "print(f\"Validation sequences: {X_val_seq.shape}, Targets: {y_val_seq.shape}\")\n",
    "print(f\"Testing sequences: {X_test_seq.shape}, Targets: {y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define the LSTM model\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layers\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "model.add(Dropout(0.2))  # Dropout for regularization\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Add a Dense output layer\n",
    "model.add(Dense(1))  # Output layer (predicts a single value)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=20,  # Number of epochs\n",
    "    batch_size=32,  # Batch size\n",
    "    verbose=1\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
