{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and Merge Datasets\n",
    "(Have merged the dataset using SQLLite because as the dataset is too large kernal crashes. But SQLLite is good in handling these type of data it is used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Merged dataset saved as 'merged_cloud_metrics.csv'\n",
      "\n",
      "Columns in the dataset:\n",
      "['timestamp', 'EC2_CPUUtilization', 'EC2_MemoryUtilization', 'EC2_DiskWriteOps', 'EC2_NetworkIn', 'RDS_CPUUtilization', 'RDS_FreeableMemory', 'RDS_DatabaseConnections', 'RDS_WriteIOPS', 'ECS_CPUUtilization', 'ECS_MemoryUtilization', 'ECS_RunningTaskCount']\n",
      "\n",
      "Sample of merged data:\n",
      "             timestamp  EC2_CPUUtilization  EC2_MemoryUtilization  \\\n",
      "0                 None                 NaN                    NaN   \n",
      "1  2023-01-01 00:00:09           63.282078                    NaN   \n",
      "2  2023-01-01 00:00:15                 NaN                    NaN   \n",
      "3  2023-01-01 00:00:31                 NaN                    NaN   \n",
      "4  2023-01-01 00:00:36           93.148608              68.979072   \n",
      "\n",
      "   EC2_DiskWriteOps  EC2_NetworkIn  RDS_CPUUtilization  RDS_FreeableMemory  \\\n",
      "0               NaN            NaN                 NaN                 NaN   \n",
      "1            3983.0     212.708467                 NaN                 NaN   \n",
      "2               NaN            NaN           30.719806           10.800739   \n",
      "3               NaN            NaN           14.019569           25.632749   \n",
      "4            2085.0     621.709971                 NaN                 NaN   \n",
      "\n",
      "   RDS_DatabaseConnections  RDS_WriteIOPS  ECS_CPUUtilization  \\\n",
      "0                      NaN            NaN                 NaN   \n",
      "1                      NaN            NaN                 NaN   \n",
      "2                      0.0            NaN                 NaN   \n",
      "3                      0.0         8240.0                 NaN   \n",
      "4                      NaN            NaN                 NaN   \n",
      "\n",
      "   ECS_MemoryUtilization  ECS_RunningTaskCount  \n",
      "0                    NaN                   NaN  \n",
      "1                    NaN                   NaN  \n",
      "2                    NaN                   NaN  \n",
      "3                    NaN                   NaN  \n",
      "4                    NaN                   NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "from datetime import datetime\n",
    "\n",
    "# Read the CSV files and convert timestamps\n",
    "ec2_df = pd.read_csv('ec2_data.csv')\n",
    "rds_df = pd.read_csv('rds_data.csv')\n",
    "ecs_df = pd.read_csv('ecs_data.csv')\n",
    "\n",
    "# Convert timestamps\n",
    "ec2_df['timestamp'] = pd.to_datetime(ec2_df['timestamp'])\n",
    "rds_df['timestamp'] = pd.to_datetime(rds_df['timestamp'])\n",
    "ecs_df['timestamp'] = pd.to_datetime(ecs_df['timestamp'])\n",
    "\n",
    "# Create database connection\n",
    "conn = sqlite3.connect('merged_metrics.db')\n",
    "\n",
    "# Store dataframes in SQLite\n",
    "ec2_df.to_sql('ec2_data', conn, if_exists='replace', index=False)\n",
    "rds_df.to_sql('rds_data', conn, if_exists='replace', index=False)\n",
    "ecs_df.to_sql('ecs_data', conn, if_exists='replace', index=False)\n",
    "\n",
    "# Query with correct column names\n",
    "query = \"\"\"\n",
    "WITH all_timestamps AS (\n",
    "    SELECT DISTINCT timestamp \n",
    "    FROM (\n",
    "        SELECT timestamp FROM ec2_data\n",
    "        UNION ALL\n",
    "        SELECT timestamp FROM rds_data\n",
    "        UNION ALL\n",
    "        SELECT timestamp FROM ecs_data\n",
    "    )\n",
    ")\n",
    "SELECT \n",
    "    at.timestamp,\n",
    "    ec2.EC2_CPUUtilization,\n",
    "    ec2.EC2_MemoryUtilization,\n",
    "    ec2.EC2_DiskWriteOps,\n",
    "    ec2.EC2_NetworkIn,\n",
    "    rds.RDS_CPUUtilization,\n",
    "    rds.RDS_FreeableMemory,\n",
    "    rds.RDS_DatabaseConnections,\n",
    "    rds.RDS_WriteIOPS,\n",
    "    ecs.ECS_CPUUtilization,\n",
    "    ecs.ECS_MemoryUtilization,\n",
    "    ecs.ECS_RunningTaskCount\n",
    "FROM all_timestamps at\n",
    "LEFT JOIN ec2_data ec2 ON at.timestamp = ec2.timestamp\n",
    "LEFT JOIN rds_data rds ON at.timestamp = rds.timestamp\n",
    "LEFT JOIN ecs_data ecs ON at.timestamp = ecs.timestamp\n",
    "ORDER BY at.timestamp;\n",
    "\"\"\"\n",
    "\n",
    "# Execute query and load results\n",
    "df = pd.read_sql_query(query, conn)\n",
    "\n",
    "# Remove any duplicate rows\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Close database connection\n",
    "conn.close()\n",
    "\n",
    "# Save the merged dataset\n",
    "df.to_csv('merged_cloud_metrics.csv', index=False)\n",
    "\n",
    "print(\"\\nMerged dataset saved as 'merged_cloud_metrics.csv'\")\n",
    "print(\"\\nColumns in the dataset:\")\n",
    "print(df.columns.tolist())\n",
    "print(\"\\nSample of merged data:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Mapped Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             timestamp  EC2_CPUUtilization  EC2_MemoryUtilization  \\\n",
      "0                  NaN                 NaN                    NaN   \n",
      "1  2023-01-01 00:00:09           63.282078                    NaN   \n",
      "2  2023-01-01 00:00:15                 NaN                    NaN   \n",
      "3  2023-01-01 00:00:31                 NaN                    NaN   \n",
      "4  2023-01-01 00:00:36           93.148608              68.979072   \n",
      "\n",
      "   EC2_DiskWriteOps  EC2_NetworkIn  RDS_CPUUtilization  RDS_FreeableMemory  \\\n",
      "0               NaN            NaN                 NaN                 NaN   \n",
      "1            3983.0     212.708467                 NaN                 NaN   \n",
      "2               NaN            NaN           30.719806           10.800739   \n",
      "3               NaN            NaN           14.019569           25.632749   \n",
      "4            2085.0     621.709971                 NaN                 NaN   \n",
      "\n",
      "   RDS_DatabaseConnections  RDS_WriteIOPS  ECS_CPUUtilization  \\\n",
      "0                      NaN            NaN                 NaN   \n",
      "1                      NaN            NaN                 NaN   \n",
      "2                      0.0            NaN                 NaN   \n",
      "3                      0.0         8240.0                 NaN   \n",
      "4                      NaN            NaN                 NaN   \n",
      "\n",
      "   ECS_MemoryUtilization  ECS_RunningTaskCount  \n",
      "0                    NaN                   NaN  \n",
      "1                    NaN                   NaN  \n",
      "2                    NaN                   NaN  \n",
      "3                    NaN                   NaN  \n",
      "4                    NaN                   NaN  \n"
     ]
    }
   ],
   "source": [
    "# Load the mapped dataset\n",
    "df = pd.read_csv('merged_cloud_metrics.csv')\n",
    "\n",
    "# Display the first few rows\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timestamp                        1\n",
      "EC2_CPUUtilization         1199400\n",
      "EC2_MemoryUtilization      1199708\n",
      "EC2_DiskWriteOps           1199367\n",
      "EC2_NetworkIn              1199426\n",
      "RDS_CPUUtilization         1198469\n",
      "RDS_FreeableMemory         1198979\n",
      "RDS_DatabaseConnections    1138788\n",
      "RDS_WriteIOPS              1199129\n",
      "ECS_CPUUtilization         1198765\n",
      "ECS_MemoryUtilization      1199061\n",
      "ECS_RunningTaskCount       1138769\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Convert timestamp to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Step 4: Handle missing values\n",
    "print(df.isnull().sum())\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Step 5: Save the cleaned dataset\n",
    "df.to_csv('cleaned_cloud_metrics.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering\n",
    "1. Temporal features\n",
    "2. Rolling Averages\n",
    "3. Lagged features\n",
    "4. Utilization ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "time data \"2023-01-01 23:25:12\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f\", at position 8333. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_cloud_metrics.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Ensure timestamp is in datetime format\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_datetime\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtimestamp\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 1. Temporal Features\u001b[39;00m\n\u001b[1;32m     10\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhour\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mhour\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:1067\u001b[0m, in \u001b[0;36mto_datetime\u001b[0;34m(arg, errors, dayfirst, yearfirst, utc, format, exact, unit, infer_datetime_format, origin, cache)\u001b[0m\n\u001b[1;32m   1065\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39mmap(cache_array)\n\u001b[1;32m   1066\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1067\u001b[0m         values \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_listlike\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1068\u001b[0m         result \u001b[38;5;241m=\u001b[39m arg\u001b[38;5;241m.\u001b[39m_constructor(values, index\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mindex, name\u001b[38;5;241m=\u001b[39marg\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1069\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, (ABCDataFrame, abc\u001b[38;5;241m.\u001b[39mMutableMapping)):\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:433\u001b[0m, in \u001b[0;36m_convert_listlike_datetimes\u001b[0;34m(arg, format, name, utc, unit, errors, dayfirst, yearfirst, exact)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# `format` could be inferred, or user didn't ask for mixed-format parsing.\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmixed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_array_strptime_with_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m result, tz_parsed \u001b[38;5;241m=\u001b[39m objects_to_datetime64(\n\u001b[1;32m    436\u001b[0m     arg,\n\u001b[1;32m    437\u001b[0m     dayfirst\u001b[38;5;241m=\u001b[39mdayfirst,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    441\u001b[0m     allow_object\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    442\u001b[0m )\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tz_parsed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;66;03m# We can take a shortcut since the datetime64 numpy array\u001b[39;00m\n\u001b[1;32m    446\u001b[0m     \u001b[38;5;66;03m# is in UTC\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/GitHub/Intelligent-resource-management-system-POC-/model_venv/lib/python3.10/site-packages/pandas/core/tools/datetimes.py:467\u001b[0m, in \u001b[0;36m_array_strptime_with_fallback\u001b[0;34m(arg, name, utc, fmt, exact, errors)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_array_strptime_with_fallback\u001b[39m(\n\u001b[1;32m    457\u001b[0m     arg,\n\u001b[1;32m    458\u001b[0m     name,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    462\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m    463\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Index:\n\u001b[1;32m    464\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;124;03m    Call array_strptime, with fallback behavior depending on 'errors'.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 467\u001b[0m     result, tz_out \u001b[38;5;241m=\u001b[39m \u001b[43marray_strptime\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfmt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexact\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexact\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mutc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mutc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tz_out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    469\u001b[0m         unit \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdatetime_data(result\u001b[38;5;241m.\u001b[39mdtype)[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32mstrptime.pyx:501\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:451\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime.array_strptime\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mstrptime.pyx:583\u001b[0m, in \u001b[0;36mpandas._libs.tslibs.strptime._parse_with_format\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: time data \"2023-01-01 23:25:12\" doesn't match format \"%Y-%m-%d %H:%M:%S.%f\", at position 8333. You might want to try:\n    - passing `format` if your strings have a consistent format;\n    - passing `format='ISO8601'` if your strings are all ISO8601 but not necessarily in exactly the same format;\n    - passing `format='mixed'`, and the format will be inferred for each element individually. You might want to use `dayfirst` alongside this."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load the preprocessed dataset\n",
    "df = pd.read_csv('cleaned_cloud_metrics.csv')\n",
    "\n",
    "# Ensure timestamp is in datetime format\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# 1. Temporal Features\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day_of_week'] = df['timestamp'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "# 2. Rolling Averages (e.g., 5-time-step rolling average for CPU and memory)\n",
    "window_size = 5\n",
    "df['cpu_rolling_avg'] = df['EC2_CPUUtilization'].rolling(window=window_size).mean()\n",
    "df['memory_rolling_avg'] = df['EC2_MemoryUtilization'].rolling(window=window_size).mean()\n",
    "\n",
    "# Fill NaN values in rolling averages (first few rows)\n",
    "df.fillna(method='bfill', inplace=True)  # Backward fill\n",
    "\n",
    "# 3. Lagged Features (e.g., CPU usage at t-1, t-2)\n",
    "df['cpu_lag_1'] = df['EC2_CPUUtilization'].shift(1)\n",
    "df['cpu_lag_2'] = df['EC2_CPUUtilization'].shift(2)\n",
    "\n",
    "# Fill NaN values in lagged features\n",
    "df.bfill(inplace=True)  # Backward fill\n",
    "\n",
    "# 4. Utilization Ratios\n",
    "df['cpu_memory_ratio'] = df['EC2_CPUUtilization'] / df['EC2_MemoryUtilization']\n",
    "df['network_in_out_ratio'] = df['EC2_NetworkIn'] / df['EC2_NetworkOut']\n",
    "\n",
    "# Handle division by zero (replace infinite values with NaN and then fill with 0)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Display the new features\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sort by timestamp (ensure data is in chronological order)\n",
    "df.sort_values('timestamp', inplace=True)\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "# For LSTM: Target could be future CPU usage (e.g., next time step)\n",
    "X = df.drop(columns=['timestamp'])  # Drop timestamp (not a feature)\n",
    "y = df['EC2_CPUUtilization']  # Example target (can be adjusted)\n",
    "\n",
    "# Time-based split (e.g., 70% train, 15% validation, 15% test)\n",
    "train_size = int(0.7 * len(df))\n",
    "val_size = int(0.15 * len(df))\n",
    "\n",
    "X_train, X_val_test = X[:train_size], X[train_size:]\n",
    "y_train, y_val_test = y[:train_size], y[train_size:]\n",
    "\n",
    "X_val, X_test = X_val_test[:val_size], X_val_test[val_size:]\n",
    "y_val, y_test = y_val_test[:val_size], y_val_test[val_size:]\n",
    "\n",
    "# Verify the splits\n",
    "print(f\"Training set size: {len(X_train)}\")\n",
    "print(f\"Validation set size: {len(X_val)}\")\n",
    "print(f\"Testing set size: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Normalize the target (if needed)\n",
    "y_train_scaled = scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_val_scaled = scaler.transform(y_val.values.reshape(-1, 1))\n",
    "y_test_scaled = scaler.transform(y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, targets, time_steps=10):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(data) - time_steps):\n",
    "        X_seq.append(data[i:i+time_steps])\n",
    "        y_seq.append(targets[i+time_steps])  # Predict the next CPU usage\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Define time steps (e.g., 10 time steps per sequence)\n",
    "time_steps = 10\n",
    "\n",
    "# Create sequences for training, validation, and testing\n",
    "X_train_seq, y_train_seq = create_sequences(X_train_scaled, y_train_scaled, time_steps)\n",
    "X_val_seq, y_val_seq = create_sequences(X_val_scaled, y_val_scaled, time_steps)\n",
    "X_test_seq, y_test_seq = create_sequences(X_test_scaled, y_test_scaled, time_steps)\n",
    "\n",
    "# Verify the shapes\n",
    "print(f\"Training sequences: {X_train_seq.shape}, Targets: {y_train_seq.shape}\")\n",
    "print(f\"Validation sequences: {X_val_seq.shape}, Targets: {y_val_seq.shape}\")\n",
    "print(f\"Testing sequences: {X_test_seq.shape}, Targets: {y_test_seq.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "\n",
    "# Define the LSTM model\n",
    "model = Sequential()\n",
    "\n",
    "# Add LSTM layers\n",
    "model.add(LSTM(100, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])))\n",
    "model.add(Dropout(0.3))  # Increased dropout rate\n",
    "model.add(LSTM(100, return_sequences=False))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "# Add a Dense output layer\n",
    "model.add(Dense(1))  # Output layer (predicts a single value)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_seq, y_train_seq,\n",
    "    validation_data=(X_val_seq, y_val_seq),\n",
    "    epochs=20,  # Number of epochs\n",
    "    batch_size=32,  # Batch size\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on the validation set\n",
    "val_loss = model.evaluate(X_val_seq, y_val_seq, verbose=0)\n",
    "print(f\"Validation Loss: {val_loss}\")\n",
    "\n",
    "# Evaluate on the testing set\n",
    "test_loss = model.evaluate(X_test_seq, y_test_seq, verbose=0)\n",
    "print(f\"Testing Loss: {test_loss}\")\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_seq)\n",
    "\n",
    "# Inverse transform the predictions and targets to original scale\n",
    "y_pred_original = scaler.inverse_transform(y_pred)\n",
    "y_test_original = scaler.inverse_transform(y_test_seq)\n",
    "\n",
    "# Display some predictions\n",
    "for i in range(5):\n",
    "    print(f\"Predicted: {y_pred_original[i][0]}, Actual: {y_test_original[i][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
