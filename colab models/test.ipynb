{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "Testing EC2 model...\n",
      "Error testing ec2 model: The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- EC2_CPUUtilization_diff_1\n",
      "- EC2_CPUUtilization_diff_3\n",
      "- EC2_CPUUtilization_lag_1\n",
      "- EC2_CPUUtilization_lag_2\n",
      "- EC2_CPUUtilization_q25\n",
      "- ...\n",
      "\n",
      "\n",
      "Testing RDS model...\n",
      "Error testing rds model: The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- RDS_CPUUtilization_diff_1\n",
      "- RDS_CPUUtilization_lag_1\n",
      "- RDS_CPUUtilization_lag_2\n",
      "- RDS_CPUUtilization_q25\n",
      "- RDS_CPUUtilization_q75\n",
      "- ...\n",
      "\n",
      "\n",
      "Testing ECS model...\n",
      "Error testing ecs model: The feature names should match those that were passed during fit.\n",
      "Feature names seen at fit time, yet now missing:\n",
      "- ECS_CPUUtilization_diff_1\n",
      "- ECS_CPUUtilization_diff_3\n",
      "- ECS_CPUUtilization_lag_1\n",
      "- ECS_CPUUtilization_lag_2\n",
      "- ECS_CPUUtilization_q25\n",
      "- ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_42428/2075511718.py:120: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df = test_df.fillna(method='ffill').fillna(method='bfill')\n",
      "/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_42428/2075511718.py:120: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df = test_df.fillna(method='ffill').fillna(method='bfill')\n",
      "/var/folders/8f/t4rh2kc96bdfdg29x5rscwmw0000gn/T/ipykernel_42428/2075511718.py:120: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df = test_df.fillna(method='ffill').fillna(method='bfill')\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import math\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import warnings\n",
    "import pickle\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Function to load and preprocess data with smoothing\n",
    "def load_and_preprocess(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Loading data from {file_path}\")\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df = df.dropna(subset=['timestamp'])\n",
    "    df = df.set_index('timestamp')\n",
    "    df = df.sort_index()\n",
    "    df = df.fillna(method='ffill').fillna(method='bfill')\n",
    "\n",
    "    for col in df.columns:\n",
    "        if 'CPUUtilization' in col:\n",
    "            df[col] = df[col].rolling(window=3, min_periods=1).mean()\n",
    "\n",
    "    print(f\"Shape after preprocessing: {df.shape}\")\n",
    "    print(f\"Index has NaT: {df.index.isna().any()}\")\n",
    "    return df\n",
    "\n",
    "# Feature engineering with feature importance analysis\n",
    "def add_features(df, service_type):\n",
    "    if service_type == 'ec2':\n",
    "        cpu_col = 'EC2_CPUUtilization'\n",
    "        memory_col = 'EC2_MemoryUtilization'\n",
    "        network_col = 'EC2_NetworkIn'\n",
    "        disk_col = 'EC2_DiskWriteOps'\n",
    "    elif service_type == 'rds':\n",
    "        cpu_col = 'RDS_CPUUtilization'\n",
    "        memory_col = 'RDS_FreeableMemory'\n",
    "        conn_col = 'RDS_DatabaseConnections'\n",
    "        io_col = 'RDS_WriteIOPS'\n",
    "    elif service_type == 'ecs':\n",
    "        cpu_col = 'ECS_CPUUtilization'\n",
    "        memory_col = 'ECS_MemoryUtilization'\n",
    "        task_col = 'ECS_RunningTaskCount'\n",
    "        network_col = 'ECS_NetworkIn'\n",
    "\n",
    "    for i in range(1, 4):\n",
    "        df[f'{cpu_col}_lag_{i}'] = df[cpu_col].shift(i)\n",
    "\n",
    "    df[f'{cpu_col}_diff_1'] = df[cpu_col].diff(1)\n",
    "    df[f'{cpu_col}_diff_3'] = df[cpu_col].diff(3)\n",
    "\n",
    "    df[f'{cpu_col}_roll_6h'] = df[cpu_col].rolling('6H', min_periods=1).mean()\n",
    "    df[f'{cpu_col}_roll_24h'] = df[cpu_col].rolling('24H', min_periods=1).mean()\n",
    "    df[f'{cpu_col}_std_6h'] = df[cpu_col].rolling('6H', min_periods=1).std()\n",
    "    df[f'{cpu_col}_q25'] = df[cpu_col].rolling('24H', min_periods=1).quantile(0.25)\n",
    "    df[f'{cpu_col}_q75'] = df[cpu_col].rolling('24H', min_periods=1).quantile(0.75)\n",
    "\n",
    "    if service_type == 'ec2':\n",
    "        df['EC2_CPU_Memory_Ratio'] = df[cpu_col] / (df[memory_col] + 1e-8)\n",
    "        df['EC2_CPU_Network_Ratio'] = df[cpu_col] / (df[network_col] + 1e-8)\n",
    "        df['EC2_CPU_Disk_Ratio'] = df[cpu_col] / (df[disk_col] + 1e-8)\n",
    "    elif service_type == 'rds':\n",
    "        df['RDS_Memory_Usage_Estimate'] = 100 - (df[memory_col] / df[memory_col].max() * 100)\n",
    "        df['RDS_CPU_Memory_Ratio'] = df[cpu_col] / (df['RDS_Memory_Usage_Estimate'] + 1e-8)\n",
    "        df['RDS_CPU_Conn_Ratio'] = df[cpu_col] / (df[conn_col] + 1e-8)\n",
    "        df['RDS_CPU_IO_Ratio'] = df[cpu_col] / (df[io_col] + 1e-8)\n",
    "    elif service_type == 'ecs':\n",
    "        df['ECS_CPU_Memory_Ratio'] = df[cpu_col] / (df[memory_col] + 1e-8)\n",
    "        df['ECS_CPU_Task_Ratio'] = df[cpu_col] / (df[task_col] + 1e-8)\n",
    "        df['ECS_CPU_Network_Ratio'] = df[cpu_col] / (df[network_col] + 1e-8)\n",
    "\n",
    "    df['hour'] = df.index.hour\n",
    "    df['day_of_week'] = df.index.dayofweek\n",
    "    df['day_of_month'] = df.index.day\n",
    "    df['month'] = df.index.month\n",
    "\n",
    "    df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "    df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "    df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['day_of_month_sin'] = np.sin(2 * np.pi * df['day_of_month'] / 31)\n",
    "    df['day_of_month_cos'] = np.cos(2 * np.pi * df['day_of_month'] / 31)\n",
    "\n",
    "    original_len = len(df)\n",
    "    df = df.dropna()\n",
    "    print(f\"Dropped {original_len - len(df)} rows containing NaN values in data columns\")\n",
    "\n",
    "    if df.index.isna().any():\n",
    "        print(f\"Found NaT in index. Cleaning index...\")\n",
    "        df = df[~df.index.isna()]\n",
    "        print(f\"Dropped rows with NaT in index. New shape: {df.shape}\")\n",
    "\n",
    "    print(\"Performing feature importance analysis...\")\n",
    "    X = df.drop(columns=[cpu_col])\n",
    "    y = df[cpu_col]\n",
    "    rf = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    rf.fit(X, y)\n",
    "    importance = pd.DataFrame({\n",
    "        'feature': X.columns,\n",
    "        'importance': rf.feature_importances_\n",
    "    }).sort_values(by='importance', ascending=False)\n",
    "    print(\"Top most important features:\")\n",
    "    print(importance.head(16))\n",
    "\n",
    "    top_features = importance['feature'].head(15).values\n",
    "    selected_columns = list(top_features) + [cpu_col]\n",
    "    df = df[selected_columns]\n",
    "\n",
    "    print(f\"Final DataFrame shape after feature selection: {df.shape}\")\n",
    "    print(f\"Index has NaT after cleaning: {df.index.isna().any()}\")\n",
    "    return df\n",
    "\n",
    "# Dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# Updated LSTM Model with GPU support\n",
    "class AttentionLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout=0.2):\n",
    "        super(AttentionLSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True,\n",
    "                           dropout=dropout if num_layers > 1 else 0, bidirectional=True)\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_size * 2, hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(hidden_size, 1)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size).to(x.device)\n",
    "        lstm_out, _ = self.lstm(x, (h0, c0))\n",
    "        attn_weights = self.attention(lstm_out)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        context = torch.sum(lstm_out * attn_weights, dim=1)\n",
    "        out = self.fc1(context)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Data preparation with reduced sequence length\n",
    "def prepare_data_for_lstm(df, target_col, sequence_length=24):\n",
    "    if df.index.isna().any():\n",
    "        raise ValueError(\"Index contains NaT values. Please clean the DataFrame before proceeding.\")\n",
    "\n",
    "    target_data = df[target_col].values.reshape(-1, 1)\n",
    "    feature_scaler = RobustScaler()\n",
    "    target_scaler = MinMaxScaler(feature_range=(0, 1))  # Explicit 0-1 range\n",
    "    scaled_target = target_scaler.fit_transform(target_data)\n",
    "    df_scaled = df.copy()\n",
    "    df_scaled[target_col] = scaled_target.flatten()\n",
    "    scaled_data = feature_scaler.fit_transform(df_scaled)\n",
    "\n",
    "    X, y = [], []\n",
    "    for i in range(len(df) - sequence_length):\n",
    "        X.append(scaled_data[i:i+sequence_length])\n",
    "        y.append(scaled_target[i+sequence_length])\n",
    "\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    split_idx = int(len(X) * 0.8)\n",
    "    X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "    y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "    X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, feature_scaler, target_scaler\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Training function with GPU support\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler=None, patience=5):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        train_losses.append(epoch_loss)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        if scheduler:\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), f'best_model_{epoch}.pth')\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                model.load_state_dict(torch.load(f'best_model_{epoch-patience}.pth'))\n",
    "                break\n",
    "\n",
    "        if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "    return model, train_losses, val_losses\n",
    "\n",
    "# Evaluation function with GPU support\n",
    "def evaluate_model(model, test_loader, criterion, target_scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item() * inputs.size(0)\n",
    "            predictions.extend(outputs.cpu().numpy())  # Move back to CPU for numpy\n",
    "            actuals.extend(targets.cpu().numpy())      # Move back to CPU for numpy\n",
    "\n",
    "    avg_test_loss = test_loss / len(test_loader.dataset)\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    smape = np.mean(2 * np.abs(predictions - actuals) / (np.abs(predictions) + np.abs(actuals) + 1e-8)) * 100\n",
    "\n",
    "    actuals_unscaled = target_scaler.inverse_transform(actuals)\n",
    "    predictions_unscaled = target_scaler.inverse_transform(predictions)\n",
    "    mae_unscaled = np.mean(np.abs(predictions_unscaled - actuals_unscaled))\n",
    "    rmse_unscaled = np.sqrt(np.mean((predictions_unscaled - actuals_unscaled)**2))\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "    print(f\"Scaled - MAE: {mae:.4f}, RMSE: {rmse:.4f}, SMAPE: {smape:.4f}%\")\n",
    "    print(f\"Unscaled - MAE: {mae_unscaled:.4f}, RMSE: {rmse_unscaled:.4f}\")\n",
    "\n",
    "    return avg_test_loss, predictions, actuals, {'mae': mae, 'rmse': rmse, 'smape': smape, 'mae_unscaled': mae_unscaled, 'rmse_unscaled': rmse_unscaled}\n",
    "\n",
    "# Plotting function (unchanged)\n",
    "def plot_results(train_losses, val_losses, predictions, actuals, service_name, metrics):\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(3, 1, 1)\n",
    "    plt.plot(train_losses, label='Training Loss')\n",
    "    plt.plot(val_losses, label='Validation Loss')\n",
    "    plt.title(f'{service_name} - Training and Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 1, 2)\n",
    "    plt.plot(actuals, label='Actual', alpha=0.7)\n",
    "    plt.plot(predictions, label='Predicted', alpha=0.7)\n",
    "    plt.title(f'{service_name} - Predictions vs Actuals\\nScaled MAE: {metrics[\"mae\"]:.4f}, RMSE: {metrics[\"rmse\"]:.4f}, SMAPE: {metrics[\"smape\"]:.2f}%\\nUnscaled MAE: {metrics[\"mae_unscaled\"]:.4f}, RMSE: {metrics[\"rmse_unscaled\"]:.4f}')\n",
    "    plt.xlabel('Time Step')\n",
    "    plt.ylabel('CPU Utilization (Scaled)')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(3, 1, 3)\n",
    "    errors = predictions - actuals\n",
    "    plt.hist(errors, bins=50, alpha=0.75)\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.title(f'{service_name} - Error Distribution')\n",
    "    plt.xlabel('Prediction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{service_name}_improved_results.png')\n",
    "    plt.show()\n",
    "\n",
    "# Forecasting function with GPU support\n",
    "def forecast_future(model, last_sequence, feature_scaler, target_scaler, steps_ahead=24, original_df=None):\n",
    "    model.eval()\n",
    "    last_sequence_tensor = torch.tensor(last_sequence, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "    future_predictions = []\n",
    "    current_sequence = last_sequence_tensor.clone()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(steps_ahead):\n",
    "            next_pred = model(current_sequence)\n",
    "            future_predictions.append(next_pred.item())\n",
    "            new_sequence = current_sequence.clone()\n",
    "            new_sequence[0, :-1, :] = current_sequence[0, 1:, :]\n",
    "            new_sequence[0, -1, 0] = next_pred.item()\n",
    "            current_sequence = new_sequence\n",
    "\n",
    "    future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "    dummy = np.zeros((len(future_predictions), 1))\n",
    "    dummy[:, 0] = future_predictions[:, 0]\n",
    "    future_predictions_original = target_scaler.inverse_transform(dummy)\n",
    "    return future_predictions_original.flatten()\n",
    "\n",
    "# Main pipeline function with GPU support\n",
    "def run_lstm_pipeline(file_path, service_type, target_col):\n",
    "    print(f\"\\nProcessing {service_type.upper()} data...\")\n",
    "\n",
    "    try:\n",
    "        df = load_and_preprocess(file_path)\n",
    "        df = add_features(df, service_type)\n",
    "        X_train, y_train, X_val, y_val, feature_scaler, target_scaler = prepare_data_for_lstm(df, target_col)\n",
    "        print(f\"Training data shape: {X_train.shape}\")\n",
    "        print(f\"Validation data shape: {X_val.shape}\")\n",
    "\n",
    "        train_dataset = TimeSeriesDataset(X_train, y_train)\n",
    "        val_dataset = TimeSeriesDataset(X_val, y_val)\n",
    "        batch_size = min(64, len(train_dataset) // 100)\n",
    "        batch_size = max(16, batch_size)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        input_size = X_train.shape[2]\n",
    "        hidden_size = 256\n",
    "        num_layers = 3\n",
    "        output_size = 1\n",
    "        dropout_rate = 0.3\n",
    "        model = AttentionLSTM(input_size, hidden_size, num_layers, output_size, dropout_rate).to(device)\n",
    "\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
    "        scheduler = ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)\n",
    "\n",
    "        print(f\"\\nTraining {service_type.upper()} LSTM model on {device}...\")\n",
    "        num_epochs = 100\n",
    "        patience = 10\n",
    "        model, train_losses, val_losses = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, scheduler, patience)\n",
    "\n",
    "        print(f\"\\nEvaluating {service_type.upper()} LSTM model...\")\n",
    "        test_loss, predictions, actuals, metrics = evaluate_model(model, val_loader, criterion, target_scaler)\n",
    "        plot_results(train_losses, val_losses, predictions, actuals, service_type, metrics)\n",
    "\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'metrics': metrics,\n",
    "            'sequence_length': 24,\n",
    "            'input_size': input_size,\n",
    "            'hidden_size': hidden_size,\n",
    "            'num_layers': num_layers\n",
    "        }, f'/content/drive/MyDrive/Hasitha/{service_type}_lstm_model_improved.pth')\n",
    "\n",
    "        with open(f'/content/drive/MyDrive/Hasitha/{service_type}_scalers.pkl', 'wb') as f:\n",
    "            pickle.dump({\n",
    "                'feature_scaler': feature_scaler,\n",
    "                'target_scaler': target_scaler\n",
    "            }, f)\n",
    "\n",
    "        print(f\"{service_type.upper()} model saved as {service_type}_lstm_model_improved.pth\")\n",
    "        print(f\"{service_type.upper()} scalers saved as {service_type}_scalers.pkl\")\n",
    "\n",
    "        print(\"\\nDemonstrating future predictions...\")\n",
    "        last_sequence = X_val[-1].cpu().numpy()  # Move back to CPU for numpy operations\n",
    "        future_steps = 24\n",
    "        future_preds = forecast_future(model, last_sequence, feature_scaler, target_scaler, steps_ahead=future_steps)\n",
    "\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(range(future_steps), future_preds, 'r-', label=f'Future {future_steps} Steps Prediction')\n",
    "        plt.title(f'{service_type.upper()} - Future CPU Utilization Forecast')\n",
    "        plt.xlabel('Steps Ahead')\n",
    "        plt.ylabel('CPU Utilization')\n",
    "        plt.legend()\n",
    "        plt.savefig(f'{service_type}_future_forecast.png')\n",
    "        plt.show()\n",
    "\n",
    "        return model, (feature_scaler, target_scaler)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR processing {service_type}: {str(e)}\")\n",
    "        return None, None\n",
    "\n",
    "# Main execution block\n",
    "if __name__ == \"__main__\":\n",
    "    services = [\n",
    "        {'file': '/content/drive/MyDrive/Hasitha/reduced_ec2_data.csv', 'type': 'ec2', 'target': 'EC2_CPUUtilization'},\n",
    "        {'file': '/content/drive/MyDrive/Hasitha/reduced_rds_data.csv', 'type': 'rds', 'target': 'RDS_CPUUtilization'},\n",
    "        {'file': '/content/drive/MyDrive/Hasitha/reduced_ecs_data.csv', 'type': 'ecs', 'target': 'ECS_CPUUtilization'}\n",
    "    ]\n",
    "\n",
    "    models = {}\n",
    "    scalers = {}\n",
    "\n",
    "    for service in services:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"Processing {service['type'].upper()} service\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        model, scaler = run_lstm_pipeline(service['file'], service['type'], service['target'])\n",
    "        if model is not None:\n",
    "            models[service['type']] = model\n",
    "            scalers[service['type']] = scaler\n",
    "            print(f\"\\nCompleted {service['type'].upper()} processing\")\n",
    "            print(f\"Model saved: {service['type']}_lstm_model_improved.pth\")\n",
    "            print(f\"Scalers saved: {service['type']}_scalers.pkl\\n\")\n",
    "        else:\n",
    "            print(f\"\\nFailed to process {service['type'].upper()}\\n\")\n",
    "\n",
    "    if models:\n",
    "        print(\"\\nThe following models and scalers have been trained and saved successfully:\")\n",
    "        for service_type in models.keys():\n",
    "            print(f\"- {service_type.upper()}:\")\n",
    "            print(f\"  Model: {service_type}_lstm_model_improved.pth\")\n",
    "            print(f\"  Scalers: {service_type}_scalers.pkl\")\n",
    "    else:\n",
    "        print(\"\\nNo models were successfully trained.\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
