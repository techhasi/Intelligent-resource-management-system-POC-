{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Deep Q-Network for Resource Scaling**\n",
    "**A DQN agent is used to decide whether to scale up, scale down, or take no action.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Define RL environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the RL Environment using CSV datasets with a fixed reward function.\n",
    "class ResourceScalingEnvCSV:\n",
    "    def __init__(self, ec2_file=\"reduced_ec2_data.csv\", \n",
    "                 rds_file=\"reduced_rds_data.csv\", \n",
    "                 ecs_file=\"reduced_ecs_data.csv\"):\n",
    "        # Load CSV files and select the appropriate CPU utilization column,\n",
    "        # converting values to floats and filling missing values with 0.\n",
    "        self.ec2_data = pd.read_csv(ec2_file)[\"EC2_CPUUtilization\"].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "        self.rds_data = pd.read_csv(rds_file)[\"RDS_CPUUtilization\"].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "        self.ecs_data = pd.read_csv(ecs_file)[\"ECS_CPUUtilization\"].apply(pd.to_numeric, errors='coerce').fillna(0).values\n",
    "\n",
    "        # Use the shortest dataset length for a consistent episode length.\n",
    "        self.length = min(len(self.ec2_data), len(self.rds_data), len(self.ecs_data))\n",
    "        self.state_size = 3  # one value per service\n",
    "        self.action_size = 3  # 0: scale up, 1: scale down, 2: no action\n",
    "        # Set target utilization lower than the typical base value (e.g., 0.4)\n",
    "        self.target = np.array([0.4, 0.4, 0.4])\n",
    "        self.index = 0\n",
    "        self.effect = np.zeros(self.state_size)\n",
    "    \n",
    "    def reset(self):\n",
    "        self.index = 0\n",
    "        self.effect = np.zeros(self.state_size)\n",
    "        base_state = np.array([self.ec2_data[self.index], \n",
    "                               self.rds_data[self.index], \n",
    "                               self.ecs_data[self.index]])\n",
    "        state = np.clip(base_state + self.effect, 0, 1)\n",
    "        return state\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Modify the effect based on action:\n",
    "        # Scale Up (action 0): reduces effective utilization (subtract 0.1)\n",
    "        # Scale Down (action 1): increases effective utilization (add 0.1)\n",
    "        # No Action (action 2): leaves effect unchanged.\n",
    "        if action == 0:\n",
    "            self.effect -= 0.1\n",
    "        elif action == 1:\n",
    "            self.effect += 0.1\n",
    "\n",
    "        self.index += 1\n",
    "        done = False\n",
    "        if self.index >= self.length:\n",
    "            done = True\n",
    "            self.index = self.length - 1  # Clamp to last index\n",
    "        \n",
    "        base_state = np.array([self.ec2_data[self.index], \n",
    "                               self.rds_data[self.index], \n",
    "                               self.ecs_data[self.index]])\n",
    "        state = np.clip(base_state + self.effect, 0, 1)\n",
    "        # Compute reward as negative average absolute error from the target, scaled by 3.\n",
    "        reward = -3 * np.mean(np.abs(state - self.target))\n",
    "        return state, reward, done"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Defining DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Define the DQN Agent with a target network and gradient clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=5000)\n",
    "        self.gamma = 0.98          # discount factor\n",
    "        self.epsilon = 1.0         # exploration rate\n",
    "        self.epsilon_min = 0.001\n",
    "        self.epsilon_decay = 0.998\n",
    "        self.learning_rate = 0.0003  # lowered learning rate\n",
    "        self.model = DQN(state_size, action_size)\n",
    "        # Create a target network and initialize it with the same weights.\n",
    "        self.target_model = DQN(state_size, action_size)\n",
    "        self.update_target_model()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def update_target_model(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice(range(self.action_size))\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action_values = self.model(state_tensor)\n",
    "        return torch.argmax(action_values).item()\n",
    "    \n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                # Use the target network for a more stable target calculation.\n",
    "                target = reward + self.gamma * torch.max(self.target_model(next_state_tensor)).item()\n",
    "            predicted = self.model(state_tensor)[0][action]\n",
    "            loss = self.criterion(predicted, torch.tensor(target).float())\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # Clip gradients to stabilize training.\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            self.optimizer.step()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Main Training Loop "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_rl_agent(episodes=2000):\n",
    "    env = ResourceScalingEnvCSV()\n",
    "    agent = DQNAgent(env.state_size, env.action_size)\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        while not done and step < env.length:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step += 1\n",
    "        agent.replay(64)\n",
    "        # Update target network every 25 episodes.\n",
    "        if (episode + 1) % 25 == 0:\n",
    "            agent.update_target_model()\n",
    "        avg_reward = total_reward / step  # average reward per step\n",
    "        if (episode+1) % 50 == 0:\n",
    "            print(f\"Episode {episode+1}/{episodes}, Avg Reward per Step: {avg_reward:.4f}, Epsilon: {agent.epsilon:.4f}\")\n",
    "    torch.save(agent.model.state_dict(), \"dqn_scaling_model.pth\")\n",
    "    print(\"DQN model saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Evaluate the Trained RL Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50/2000, Avg Reward per Step: -1.6182, Epsilon: 0.9047\n",
      "Episode 100/2000, Avg Reward per Step: -1.2136, Epsilon: 0.8186\n",
      "Episode 150/2000, Avg Reward per Step: -1.8000, Epsilon: 0.7406\n",
      "Episode 200/2000, Avg Reward per Step: -1.7670, Epsilon: 0.6701\n",
      "Episode 250/2000, Avg Reward per Step: -1.7537, Epsilon: 0.6062\n",
      "Episode 300/2000, Avg Reward per Step: -1.2063, Epsilon: 0.5485\n",
      "Episode 350/2000, Avg Reward per Step: -1.3192, Epsilon: 0.4962\n",
      "Episode 400/2000, Avg Reward per Step: -1.2048, Epsilon: 0.4490\n",
      "Episode 450/2000, Avg Reward per Step: -1.8000, Epsilon: 0.4062\n",
      "Episode 500/2000, Avg Reward per Step: -1.2076, Epsilon: 0.3675\n",
      "Episode 550/2000, Avg Reward per Step: -1.8000, Epsilon: 0.3325\n",
      "Episode 600/2000, Avg Reward per Step: -1.8000, Epsilon: 0.3008\n",
      "Episode 650/2000, Avg Reward per Step: -1.6706, Epsilon: 0.2722\n",
      "Episode 700/2000, Avg Reward per Step: -1.5554, Epsilon: 0.2463\n",
      "Episode 750/2000, Avg Reward per Step: -1.7937, Epsilon: 0.2228\n",
      "Episode 800/2000, Avg Reward per Step: -1.7995, Epsilon: 0.2016\n",
      "Episode 850/2000, Avg Reward per Step: -1.2033, Epsilon: 0.1824\n",
      "Episode 900/2000, Avg Reward per Step: -1.8000, Epsilon: 0.1650\n",
      "Episode 950/2000, Avg Reward per Step: -1.2031, Epsilon: 0.1493\n",
      "Episode 1000/2000, Avg Reward per Step: -1.7384, Epsilon: 0.1351\n",
      "Episode 1050/2000, Avg Reward per Step: -1.7713, Epsilon: 0.1222\n",
      "Episode 1100/2000, Avg Reward per Step: -1.6954, Epsilon: 0.1106\n",
      "Episode 1150/2000, Avg Reward per Step: -1.2031, Epsilon: 0.1000\n",
      "Episode 1200/2000, Avg Reward per Step: -1.2039, Epsilon: 0.0905\n",
      "Episode 1250/2000, Avg Reward per Step: -1.2053, Epsilon: 0.0819\n",
      "Episode 1300/2000, Avg Reward per Step: -1.2029, Epsilon: 0.0741\n",
      "Episode 1350/2000, Avg Reward per Step: -1.2029, Epsilon: 0.0670\n",
      "Episode 1400/2000, Avg Reward per Step: -1.8000, Epsilon: 0.0606\n",
      "Episode 1450/2000, Avg Reward per Step: -1.2028, Epsilon: 0.0549\n",
      "Episode 1500/2000, Avg Reward per Step: -1.2029, Epsilon: 0.0496\n",
      "Episode 1550/2000, Avg Reward per Step: -1.8000, Epsilon: 0.0449\n",
      "Episode 1600/2000, Avg Reward per Step: -1.2037, Epsilon: 0.0406\n",
      "Episode 1650/2000, Avg Reward per Step: -1.8000, Epsilon: 0.0368\n",
      "Episode 1700/2000, Avg Reward per Step: -1.7623, Epsilon: 0.0333\n",
      "Episode 1750/2000, Avg Reward per Step: -1.5641, Epsilon: 0.0301\n",
      "Episode 1800/2000, Avg Reward per Step: -1.5837, Epsilon: 0.0272\n",
      "Episode 1850/2000, Avg Reward per Step: -1.2027, Epsilon: 0.0246\n",
      "Episode 1900/2000, Avg Reward per Step: -1.7963, Epsilon: 0.0223\n",
      "Episode 1950/2000, Avg Reward per Step: -1.2028, Epsilon: 0.0202\n",
      "Episode 2000/2000, Avg Reward per Step: -1.7551, Epsilon: 0.0182\n",
      "DQN model saved!\n",
      "Average Reward per Step over evaluation episodes: -1.7196\n",
      "Action Distribution: {0: 0.3333002, 1: 0.3332184, 2: 0.3334814}\n",
      "MAE: 1.7196, MSE: 2.9619, RMSE: 1.7210\n"
     ]
    }
   ],
   "source": [
    "def evaluate_rl_agent(episodes=100):\n",
    "    env = ResourceScalingEnvCSV()\n",
    "    agent = DQNAgent(env.state_size, env.action_size)\n",
    "    agent.model.load_state_dict(torch.load(\"dqn_scaling_model.pth\"))\n",
    "    agent.model.eval()\n",
    "    \n",
    "    total_rewards = []\n",
    "    action_counts = {0: 0, 1: 0, 2: 0}\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        total_reward = 0\n",
    "        step = 0\n",
    "        done = False\n",
    "        while not done and step < env.length:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            total_reward += reward\n",
    "            action_counts[action] += 1\n",
    "            state = next_state\n",
    "            step += 1\n",
    "        avg_reward = total_reward / step\n",
    "        total_rewards.append(avg_reward)\n",
    "    \n",
    "    overall_avg = np.mean(total_rewards)\n",
    "    action_distribution = {k: v / sum(action_counts.values()) for k, v in action_counts.items()}\n",
    "    \n",
    "    mae = mean_absolute_error(total_rewards, np.zeros_like(total_rewards))\n",
    "    mse = mean_squared_error(total_rewards, np.zeros_like(total_rewards))\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    print(f\"Average Reward per Step over evaluation episodes: {overall_avg:.4f}\")\n",
    "    print(f\"Action Distribution: {action_distribution}\")\n",
    "    print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}\")\n",
    "    \n",
    "    return overall_avg, action_distribution, mae, mse, rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Run training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train_rl_agent()\n",
    "    evaluate_rl_agent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
