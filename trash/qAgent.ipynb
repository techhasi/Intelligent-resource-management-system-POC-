{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b0ca71e",
   "metadata": {},
   "source": [
    "Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe1b4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, bins_per_dim=6, action_size_per_service=3, num_services=3):\n",
    "        self.bins_per_dim = bins_per_dim  \n",
    "        self.action_size_per_service = action_size_per_service\n",
    "        self.num_services = num_services\n",
    "        \n",
    "        self.q_table = defaultdict(float)\n",
    "        \n",
    "        self.eligibility_traces = defaultdict(float)\n",
    "        self.lambda_param = 0.7  \n",
    "        \n",
    "        self.epsilon = 0.3  \n",
    "        self.epsilon_min = 0.05\n",
    "        self.epsilon_decay = 0.99\n",
    "        self.steps_done = 0\n",
    "        \n",
    "        self.initial_lr = 0.08\n",
    "        self.min_lr = 0.01\n",
    "        \n",
    "        self.reward_history = []\n",
    "        self.epsilon_history = []\n",
    "        self.loss_history = []\n",
    "        self.service_states = {i: {} for i in range(num_services)}\n",
    "        self.action_counts = {i: {0: 0, 1: 0, 2: 0} for i in range(num_services)}\n",
    "        self.high_cpu_actions = {i: {0: 0, 1: 0, 2: 0} for i in range(num_services)}\n",
    "        self.service_performance = {i: {'rewards': [], 'actions': []} for i in range(num_services)}\n",
    "        \n",
    "        self.service_thresholds = {\n",
    "            0: {'low': 15, 'high': 85, 'opt_low': 30, 'opt_high': 70},  \n",
    "            1: {'low': 18, 'high': 80, 'opt_low': 35, 'opt_high': 65},  \n",
    "            2: {'low': 15, 'high': 85, 'opt_low': 30, 'opt_high': 70} \n",
    "        }\n",
    "        \n",
    "        self._initialize_q_table()\n",
    "        \n",
    "        self.state_action_cache = {}\n",
    "    \n",
    "    def _initialize_q_table(self):\n",
    "        print(\"Initializing Q-table with guided values...\")\n",
    "        for cpu_level in range(0, 101, 5):  \n",
    "            for resources in range(1, 11):  \n",
    "                for service_idx in range(self.num_services):\n",
    "                    thresholds = self.service_thresholds[service_idx]\n",
    "                    \n",
    "                    state_key = (resources, cpu_level, service_idx)\n",
    "                    \n",
    "                    if cpu_level >= thresholds['high']:\n",
    "                        self.q_table[(state_key, 0)] = -20.0  \n",
    "                        self.q_table[(state_key, 1)] = 0.0    \n",
    "                        self.q_table[(state_key, 2)] = 20.0   \n",
    "                    elif cpu_level <= thresholds['low'] and resources > 1:\n",
    "                        self.q_table[(state_key, 0)] = 10.0   \n",
    "                        self.q_table[(state_key, 1)] = 0.0    \n",
    "                        self.q_table[(state_key, 2)] = -10.0 \n",
    "                    elif cpu_level <= thresholds['low'] and resources <= 1:\n",
    "                        \n",
    "                        self.q_table[(state_key, 0)] = -15.0  \n",
    "                        self.q_table[(state_key, 1)] = 10.0  \n",
    "                        self.q_table[(state_key, 2)] = -5.0  \n",
    "                    elif thresholds['opt_low'] <= cpu_level <= thresholds['opt_high']:\n",
    "\n",
    "                        self.q_table[(state_key, 0)] = -5.0  \n",
    "                        self.q_table[(state_key, 1)] = 10.0   \n",
    "                        self.q_table[(state_key, 2)] = -5.0  \n",
    "                    elif thresholds['opt_high'] < cpu_level < thresholds['high']:\n",
    "\n",
    "                        self.q_table[(state_key, 0)] = -10.0 \n",
    "                        self.q_table[(state_key, 1)] = 5.0    \n",
    "                        self.q_table[(state_key, 2)] = 8.0   \n",
    "                    else: \n",
    "\n",
    "                        if resources > 1:\n",
    "                            self.q_table[(state_key, 0)] = 8.0    \n",
    "                            self.q_table[(state_key, 1)] = 5.0\n",
    "                            self.q_table[(state_key, 2)] = -8.0   \n",
    "                        else:\n",
    "                            self.q_table[(state_key, 0)] = -10.0 \n",
    "                            self.q_table[(state_key, 1)] = 10.0   \n",
    "                            self.q_table[(state_key, 2)] = -5.0   \n",
    "        \n",
    "        print(\"Q-table initialization complete.\")\n",
    "    \n",
    "    def get_adaptive_learning_rate(self):\n",
    "        # Adaptive learning rate function\n",
    "        lr = max(self.min_lr, self.initial_lr * (0.99 ** (self.steps_done // 300)))\n",
    "        \n",
    "\n",
    "        if len(self.reward_history) > 10:\n",
    "            recent_rewards = self.reward_history[-10:]\n",
    "            if np.mean(recent_rewards[-3:]) > np.mean(recent_rewards[:7]):\n",
    "                lr *= 1.05  # Increase if improving\n",
    "            elif np.mean(recent_rewards[-3:]) < np.mean(recent_rewards[:7]):\n",
    "                lr *= 0.95  # Decrease if getting worse\n",
    "        \n",
    "        return np.clip(lr, self.min_lr, self.initial_lr)\n",
    "    \n",
    "    def get_simplified_state(self, state, service_idx):\n",
    "        try:\n",
    "\n",
    "            if service_idx < len(state) // 2:\n",
    "                resources = state[service_idx]\n",
    "                cpu_util = state[service_idx + 3] * 100  # Denormalize\n",
    "                \n",
    "\n",
    "                resources = int(min(round(resources), 10))  # Cap at 10 resources\n",
    "                cpu_level = int(min(round(cpu_util), 100))  # Cap at 100%\n",
    "                \n",
    "\n",
    "                return (resources, cpu_level, service_idx)\n",
    "            else:\n",
    "\n",
    "                return (2, 50, service_idx)\n",
    "        except Exception as e:\n",
    "            print(f\"Error in get_simplified_state: {e}\")\n",
    "\n",
    "            return (2, 50, service_idx)\n",
    "    \n",
    "    def get_q_value(self, state_tuple, service_idx, action):\n",
    "        key = (state_tuple, action)\n",
    "        return self.q_table[key]\n",
    "    \n",
    "    def set_q_value(self, state_tuple, action, value):\n",
    "        key = (state_tuple, action)\n",
    "        self.q_table[key] = float(value)\n",
    "    \n",
    "    def select_action(self, state):\n",
    "        try:\n",
    "\n",
    "            for i in range(self.num_services):\n",
    "                service_state = {}\n",
    "                if i < len(state) // 2:\n",
    "                    service_state['resources'] = state[i]\n",
    "                    service_state['cpu'] = state[i + 3] * 100 \n",
    "                    self.service_states[i] = service_state\n",
    "            \n",
    "\n",
    "            actions = {}\n",
    "            \n",
    "\n",
    "            for i in range(self.num_services):\n",
    "\n",
    "                cpu_util = self.service_states[i].get('cpu', 50)\n",
    "                resources = self.service_states[i].get('resources', 2)\n",
    "                simple_state = self.get_simplified_state(state, i)\n",
    "                \n",
    "\n",
    "                thresholds = self.service_thresholds[i]\n",
    "                \n",
    "\n",
    "                if random.random() > self.epsilon:\n",
    "\n",
    "                    if cpu_util >= thresholds['high']:\n",
    "\n",
    "                        actions[i] = 2\n",
    "                    elif cpu_util <= thresholds['low'] and resources > 1:\n",
    "\n",
    "                        actions[i] = 0\n",
    "                    elif cpu_util <= thresholds['low'] and resources <= 1:\n",
    "\n",
    "                        actions[i] = 1  # No change\n",
    "                    elif thresholds['opt_low'] <= cpu_util <= thresholds['opt_high']:\n",
    "\n",
    "                        actions[i] = 1\n",
    "                    else:\n",
    "\n",
    "\n",
    "                        q_values = [self.get_q_value(simple_state, i, a) for a in range(3)]\n",
    "                        \n",
    "\n",
    "                        if thresholds['opt_high'] < cpu_util < thresholds['high']:\n",
    "\n",
    "                            q_values[0] -= 5.0  \n",
    "                            q_values[2] += (cpu_util - thresholds['opt_high']) / 10.0  \n",
    "                        elif thresholds['low'] < cpu_util < thresholds['opt_low']:\n",
    "\n",
    "                            if resources > 1:\n",
    "                                q_values[0] += (thresholds['opt_low'] - cpu_util) / 10.0 \n",
    "                                q_values[2] -= 5.0\n",
    "                        \n",
    "\n",
    "                        best_action = np.argmax(q_values)\n",
    "                        actions[i] = best_action\n",
    "                else:\n",
    "\n",
    "                    if cpu_util >= thresholds['high']:\n",
    "\n",
    "                        probs = [0.0, 0.05, 0.95]  \n",
    "                    elif cpu_util <= thresholds['low'] and resources > 1:\n",
    "\n",
    "                        probs = [0.95, 0.05, 0.0] \n",
    "                    elif thresholds['opt_low'] <= cpu_util <= thresholds['opt_high']:\n",
    "\n",
    "                        probs = [0.05, 0.9, 0.05]  \n",
    "                    elif thresholds['opt_high'] < cpu_util < thresholds['high']:\n",
    "\n",
    "                        scale_factor = (cpu_util - thresholds['opt_high']) / (thresholds['high'] - thresholds['opt_high'])\n",
    "                        up_prob = 0.3 + 0.6 * scale_factor \n",
    "                        probs = [0.0, 1.0 - up_prob, up_prob]  \n",
    "                    else: \n",
    "\n",
    "                        if resources > 1:\n",
    "                            scale_factor = (thresholds['opt_low'] - cpu_util) / (thresholds['opt_low'] - thresholds['low'])\n",
    "                            down_prob = 0.3 + 0.6 * scale_factor \n",
    "                            probs = [down_prob, 1.0 - down_prob, 0.0]  \n",
    "                        else:\n",
    "\n",
    "                            probs = [0.0, 0.9, 0.1]  \n",
    "                    \n",
    "\n",
    "                    if resources <= 1:\n",
    "                        probs[0] = 0.0  \n",
    "\n",
    "                        total = sum(probs)\n",
    "                        if total > 0:\n",
    "                            probs = [p/total for p in probs]\n",
    "                        else:\n",
    "                            probs = [0.0, 0.9, 0.1] \n",
    "                    \n",
    "\n",
    "                    action = random.choices([0, 1, 2], weights=probs)[0]\n",
    "                    actions[i] = action\n",
    "                \n",
    "\n",
    "                self.action_counts[i][actions[i]] += 1\n",
    "                if cpu_util > 80:\n",
    "                    self.high_cpu_actions[i][actions[i]] += 1\n",
    "            \n",
    "            return actions\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in select_action: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "\n",
    "            return {i: 1 for i in range(self.num_services)}\n",
    "    \n",
    "    def update_q_table(self, state, actions, reward, next_state):\n",
    "        try:\n",
    "\n",
    "            learning_rate = self.get_adaptive_learning_rate()\n",
    "            \n",
    "\n",
    "            if np.isnan(reward):\n",
    "                reward = 0.0\n",
    "            \n",
    "\n",
    "            for i in range(self.num_services):\n",
    "                if i in actions:\n",
    "                    action = actions[i]\n",
    "                    \n",
    "\n",
    "                    simple_state = self.get_simplified_state(state, i)\n",
    "                    next_simple_state = self.get_simplified_state(next_state, i)\n",
    "                    \n",
    "\n",
    "                    cpu_util = self.service_states[i].get('cpu', 50)\n",
    "                    resources = self.service_states[i].get('resources', 2)\n",
    "                    thresholds = self.service_thresholds[i]\n",
    "                    \n",
    "\n",
    "                    followed_rules = False\n",
    "                    if cpu_util >= thresholds['high'] and action == 2:\n",
    "\n",
    "                        followed_rules = True\n",
    "                    elif cpu_util <= thresholds['low'] and resources > 1 and action == 0:\n",
    "\n",
    "                        followed_rules = True\n",
    "                    elif thresholds['opt_low'] <= cpu_util <= thresholds['opt_high'] and action == 1:\n",
    "\n",
    "                        followed_rules = True\n",
    "                    \n",
    "\n",
    "                    effective_reward = reward / self.num_services\n",
    "                    if followed_rules:\n",
    "\n",
    "                        effective_reward += 2.0\n",
    "                    elif (cpu_util >= thresholds['high'] and action == 0) or \\\n",
    "                         (cpu_util <= thresholds['low'] and resources > 1 and action == 2):\n",
    "\n",
    "                        effective_reward -= 5.0\n",
    "                    \n",
    "\n",
    "                    current_q = self.get_q_value(simple_state, i, action)\n",
    "                    \n",
    "\n",
    "                    next_q_values = [self.get_q_value(next_simple_state, i, a) for a in range(3)]\n",
    "                    max_next_q = max(next_q_values)\n",
    "                    \n",
    "\n",
    "                    td_error = effective_reward + 0.9 * max_next_q - current_q\n",
    "                    td_error = np.clip(td_error, -5.0, 5.0)  # Prevent large updates\n",
    "                    \n",
    "\n",
    "                    new_q = current_q + learning_rate * td_error\n",
    "                    \n",
    "\n",
    "                    if followed_rules:\n",
    "\n",
    "                        original_rule_value = self.get_initial_q_value(simple_state, i, action)\n",
    "                        new_q = 0.7 * original_rule_value + 0.3 * new_q\n",
    "                    \n",
    "\n",
    "                    if np.isfinite(new_q):\n",
    "                        self.set_q_value(simple_state, action, new_q)\n",
    "            \n",
    "\n",
    "            self.reward_history.append(reward)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in update_q_table: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "    \n",
    "    def get_initial_q_value(self, state_tuple, service_idx, action):\n",
    "\n",
    "        resources, cpu_level, _ = state_tuple\n",
    "        thresholds = self.service_thresholds[service_idx]\n",
    "        \n",
    "\n",
    "        if cpu_level >= thresholds['high']:\n",
    "\n",
    "            return [-20.0, 0.0, 20.0][action]\n",
    "        elif cpu_level <= thresholds['low'] and resources > 1:\n",
    "\n",
    "            return [10.0, 0.0, -10.0][action]\n",
    "        elif cpu_level <= thresholds['low'] and resources <= 1:\n",
    "\n",
    "            return [-15.0, 10.0, -5.0][action]\n",
    "        elif thresholds['opt_low'] <= cpu_level <= thresholds['opt_high']:\n",
    "\n",
    "            return [-5.0, 10.0, -5.0][action]\n",
    "        elif thresholds['opt_high'] < cpu_level < thresholds['high']:\n",
    "\n",
    "            return [-10.0, 5.0, 8.0][action]\n",
    "        else: \n",
    "            if resources > 1:\n",
    "                return [8.0, 5.0, -8.0][action]\n",
    "            else:\n",
    "                return [-10.0, 10.0, -5.0][action]\n",
    "    \n",
    "    def update_epsilon(self):\n",
    "\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "        self.epsilon_history.append(self.epsilon)\n",
    "        self.steps_done += 1\n",
    "    \n",
    "    def save_model(self, file_path):\n",
    "        try:\n",
    "            model_data = {\n",
    "                'q_table': dict(self.q_table),\n",
    "                'epsilon': self.epsilon,\n",
    "                'reward_history': self.reward_history,\n",
    "                'epsilon_history': self.epsilon_history,\n",
    "                'bins_per_dim': self.bins_per_dim,\n",
    "                'action_size_per_service': self.action_size_per_service,\n",
    "                'num_services': self.num_services,\n",
    "                'loss_history': self.loss_history,\n",
    "                'action_counts': self.action_counts,\n",
    "                'high_cpu_actions': self.high_cpu_actions,\n",
    "                'service_performance': self.service_performance,\n",
    "                'service_thresholds': self.service_thresholds\n",
    "            }\n",
    "            \n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(model_data, f)\n",
    "            \n",
    "            print(f\"Model saved to {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving model: {e}\")\n",
    "            print(traceback.format_exc())\n",
    "    \n",
    "    def load_model(self, file_path):\n",
    "        if os.path.exists(file_path):\n",
    "            try:\n",
    "                with open(file_path, 'rb') as f:\n",
    "                    model_data = pickle.load(f)\n",
    "                \n",
    "                self.q_table = defaultdict(float, model_data['q_table'])\n",
    "                self.epsilon = model_data.get('epsilon', self.epsilon_min)\n",
    "                self.reward_history = model_data.get('reward_history', [])\n",
    "                self.epsilon_history = model_data.get('epsilon_history', [])\n",
    "                self.bins_per_dim = model_data.get('bins_per_dim', self.bins_per_dim)\n",
    "                self.loss_history = model_data.get('loss_history', [])\n",
    "                self.action_counts = model_data.get('action_counts', \n",
    "                                               {i: {0: 0, 1: 0, 2: 0} for i in range(self.num_services)})\n",
    "                self.high_cpu_actions = model_data.get('high_cpu_actions', \n",
    "                                                  {i: {0: 0, 1: 0, 2: 0} for i in range(self.num_services)})\n",
    "                self.service_performance = model_data.get('service_performance',\n",
    "                                                     {i: {'rewards': [], 'actions': []} for i in range(self.num_services)})\n",
    "                self.service_thresholds = model_data.get('service_thresholds', self.service_thresholds)\n",
    "                \n",
    "                print(f\"Model loaded from {file_path}\")\n",
    "                return True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading model: {e}\")\n",
    "                print(traceback.format_exc())\n",
    "                return False\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6066333",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52934673",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_agent(env, agent, num_episodes=50, save_path='cloud_dqn.pkl', save_best=True):\n",
    "    episode_rewards = []\n",
    "    best_reward = float('-inf')\n",
    "    best_episode = 0\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    all_resources = []\n",
    "    all_cpu = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        done = False\n",
    "        \n",
    "\n",
    "        episode_resources = {service: [] for service in env.services}\n",
    "        episode_cpu = {service: [] for service in env.services}\n",
    "        \n",
    "\n",
    "        print(f\"\\nStarting Episode {episode+1}/{num_episodes}...\")\n",
    "        sys.stdout.flush() \n",
    "        \n",
    "        while not done:\n",
    "\n",
    "            actions = agent.select_action(state)\n",
    "            \n",
    "\n",
    "            for i, service in enumerate(env.services):\n",
    "                episode_resources[service].append(env.current_resources[service])\n",
    "                episode_cpu[service].append(env.current_cpu[service])\n",
    "            \n",
    "\n",
    "            next_state, reward, done, info = env.step(actions)\n",
    "            \n",
    "\n",
    "            agent.update_q_table(state, actions, reward, next_state)\n",
    "            \n",
    "\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            steps += 1\n",
    "            \n",
    "\n",
    "            if steps > 0 and steps % 10 == 0:\n",
    "                print(f\"  Step {steps}, Reward so far: {episode_reward:.2f}\")\n",
    "                sys.stdout.flush()\n",
    "        \n",
    "\n",
    "        all_resources.append(episode_resources)\n",
    "        all_cpu.append(episode_cpu)\n",
    "        \n",
    "\n",
    "        agent.update_epsilon()\n",
    "        \n",
    "\n",
    "        episode_rewards.append(episode_reward)\n",
    "        agent.reward_history.append(episode_reward)\n",
    "        \n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "\n",
    "        print(f\"Episode {episode+1}/{num_episodes} completed | Steps: {steps} | \"\n",
    "              f\"Reward: {episode_reward:.2f} | Epsilon: {agent.epsilon:.2f} | \"\n",
    "              f\"Time: {elapsed_time:.1f}s | Q-table size: {len(agent.q_table)}\")\n",
    "        \n",
    "\n",
    "        if save_best and episode_reward > best_reward:\n",
    "            best_reward = episode_reward\n",
    "            best_episode = episode + 1\n",
    "            agent.save_model(save_path.replace('.pkl', '_best.pkl'))\n",
    "            print(f\"New best model saved with reward: {best_reward:.2f}\")\n",
    "        \n",
    "\n",
    "        if (episode + 1) % 10 == 0:\n",
    "            agent.save_model(save_path.replace('.pkl', f'_ep{episode+1}.pkl'))\n",
    "            \n",
    "\n",
    "            if episode > 0 and (episode + 1) % 20 == 0:\n",
    "                visualize_training_progress(episode_rewards, agent, all_resources, all_cpu, episode)\n",
    "    \n",
    "\n",
    "    agent.save_model(save_path)\n",
    "    print(f\"Final model saved to {save_path}\")\n",
    "    print(f\"Best model was from episode {best_episode} with reward {best_reward:.2f}\")\n",
    "    \n",
    "\n",
    "    plot_training_metrics(agent, episode_rewards)\n",
    "    \n",
    "    return agent, best_episode, best_reward\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
